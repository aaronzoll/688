{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3392793-46ec-47ec-b633-d695ab711310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>Perimeter</th>\n",
       "      <th>Compactness</th>\n",
       "      <th>KernelLen</th>\n",
       "      <th>KernelWidth</th>\n",
       "      <th>AsymmetryCoef</th>\n",
       "      <th>KernelGrooveLen</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.26</td>\n",
       "      <td>14.84</td>\n",
       "      <td>0.8710</td>\n",
       "      <td>5.763</td>\n",
       "      <td>3.312</td>\n",
       "      <td>2.221</td>\n",
       "      <td>5.220</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.88</td>\n",
       "      <td>14.57</td>\n",
       "      <td>0.8811</td>\n",
       "      <td>5.554</td>\n",
       "      <td>3.333</td>\n",
       "      <td>1.018</td>\n",
       "      <td>4.956</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.29</td>\n",
       "      <td>14.09</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>5.291</td>\n",
       "      <td>3.337</td>\n",
       "      <td>2.699</td>\n",
       "      <td>4.825</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.84</td>\n",
       "      <td>13.94</td>\n",
       "      <td>0.8955</td>\n",
       "      <td>5.324</td>\n",
       "      <td>3.379</td>\n",
       "      <td>2.259</td>\n",
       "      <td>4.805</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.14</td>\n",
       "      <td>14.99</td>\n",
       "      <td>0.9034</td>\n",
       "      <td>5.658</td>\n",
       "      <td>3.562</td>\n",
       "      <td>1.355</td>\n",
       "      <td>5.175</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Area  Perimeter  Compactness  KernelLen  KernelWidth  AsymmetryCoef  \\\n",
       "0  15.26      14.84       0.8710      5.763        3.312          2.221   \n",
       "1  14.88      14.57       0.8811      5.554        3.333          1.018   \n",
       "2  14.29      14.09       0.9050      5.291        3.337          2.699   \n",
       "3  13.84      13.94       0.8955      5.324        3.379          2.259   \n",
       "4  16.14      14.99       0.9034      5.658        3.562          1.355   \n",
       "\n",
       "   KernelGrooveLen  Class  \n",
       "0            5.220      1  \n",
       "1            4.956      1  \n",
       "2            4.825      1  \n",
       "3            4.805      1  \n",
       "4            5.175      1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Compute neural network classification rate.\n",
    "#\n",
    "#   Inputs: \n",
    "#      Xtrain = training feature data\n",
    "#               numpy array rows are obs and columns are features\n",
    "#      Ytrain = vector of coded classes coded 0,1,...,nclasses-1     \n",
    "#      Xtest = test feature data\n",
    "#      Ytest =  vector of coded classes (labels need to be 0,1,...,nclasses-1)\n",
    "#      hidden_layer_sizes = list with numbers of nodes in hidden layers\n",
    "#      batchsize = number of obs in each batch\n",
    "#      nepochs = number of epochs to train on\n",
    "#      lrate = learning rate to use to train\n",
    "#\n",
    "#\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "#\n",
    "# This function takes as input the following:\n",
    "#\n",
    "#   Xtrain = training data predictor variables\n",
    "#            a np 2d array with rows=observations and columns=variables\n",
    "#   Ytrain = training data response variable a np array 1d array\n",
    "#   Xtest = test predictor vars\n",
    "#   Ytest = test response var\n",
    "#   hidden_layer_sizes = list with the number of nodes at intermediate layers\n",
    "#   batchsize = observations are broken up into batches so this is number of obs/batch\n",
    "#   nepochs = when fitting, we go through the entire dataset nepochs times\n",
    "#   lrate = parameter that determine stepsize (this is a bit complicated to describe for the \n",
    "#           optimizer employed (Adam)\n",
    "#\n",
    "def NeuralNetwork(Xtrain,Ytrain,Xtest,Ytest,hidden_layer_sizes,batchsize, nepochs,lrate):\n",
    "    nobs,nfeatures=Xtrain.shape\n",
    "    ntest=Xtest.shape[0]\n",
    "    nclasses=len(set(Ytrain))\n",
    "    \n",
    "    #device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device=\"cpu\" # don't use gpu - when I run this in jupyter I get an error that does not come up \n",
    "    \n",
    "    #\n",
    "    # Dataset is a torch class - we **inherit** from this class to make our own \n",
    "    # custom MyDataset objects. The algorithm for fitting a neural network to the \n",
    "    # data needs a couple of functions.\n",
    "    #\n",
    "    # 1) __len__() giving the number of observations in the dataset\n",
    "    # 2) __getitem__() - enabling one to get a pair/row of X,Y values by using square brackets\n",
    "    # \n",
    "    class MyDataSet(Dataset):\n",
    "        def __init__(self,X,Y):\n",
    "            self.X=X\n",
    "            self.Y=Y\n",
    "            self.N=self.X.shape[0]\n",
    "            self.K=self.X.shape[1]\n",
    "        def __len__(self):\n",
    "            return(self.N)\n",
    "        def __getitem__(self, idx):\n",
    "            return self.X[idx],self.Y[idx]\n",
    "    #\n",
    "    # Here we construct two Dataset objects\n",
    "    #\n",
    "    mydata_train=MyDataSet(Xtrain,Ytrain)\n",
    "    mydata_test=MyDataSet(Xtest,Ytest)\n",
    "    #\n",
    "    # Create a network inheriting from torch.nn.Module\n",
    "    # I've introduced my own constructor.\n",
    "    #\n",
    "    class MyNetwork(torch.nn.Module):\n",
    "        #\n",
    "        # the constructor takes as arguments:\n",
    "        # \n",
    "        #    number of features\n",
    "        #    number of classes\n",
    "        #    a list of sizes of numbers of nodes in the hidden layers\n",
    "        #\n",
    "        def __init__(self,nfeatures,nclasses, hiddenlayersizes):\n",
    "            nhiddenlayers=len(hiddenlayersizes)\n",
    "            #\n",
    "            # the following is used since the constructor for the base class\n",
    "            # was over-ridden, but we still need some attributes that\n",
    "            # are contained in the super class (torch.nn.Module)\n",
    "            #\n",
    "            super().__init__()\n",
    "            #\n",
    "            # create the layers\n",
    "            #\n",
    "            if nhiddenlayers==0:\n",
    "                layers=[]\n",
    "                layers.append(nn.Linear(nfeatures,nclasses))\n",
    "            else:\n",
    "                layers=[]\n",
    "                layers.append(nn.Linear(nfeatures,hiddenlayersizes[0])) # fully connected\n",
    "                layers.append(nn.Sigmoid()) # activation function \n",
    "                for i in range(1,nhiddenlayers):\n",
    "                    layers.append(nn.Linear(hiddenlayersizes[i-1],hiddenlayersizes[i]))\n",
    "                    layers.append(nn.Sigmoid()) # activation\n",
    "                layers.append(nn.Linear(hiddenlayersizes[nhiddenlayers-1],nclasses))\n",
    "                #\n",
    "                # note that when using cross-entropy loss function \n",
    "                # we don't need to include an activation function after the last linear\n",
    "                # transformation\n",
    "                #\n",
    "                # cross entropy loss calculates the output to exp(output)/sum(exp(output))\n",
    "                # to convert to probability vector and compares with true class value\n",
    "                #\n",
    "                self.layers=nn.ModuleList(layers)\n",
    "        #\n",
    "        # Calculates the output from the input feature vector\n",
    "        # This function is needed when model() is called.\n",
    "        #\n",
    "        def forward(self, x):\n",
    "            for L in self.layers:\n",
    "                x=L(x)\n",
    "            return x\n",
    "        #\n",
    "        # Function to compute output probabilities from input.\n",
    "        # This should be the same as y=model(x) followed by softmax\n",
    "        # i.e. exp(y)/sum(exp(y))\n",
    "        #\n",
    "        def probability_vector(self,x):\n",
    "            for L in self.layers:\n",
    "                x=L(x)\n",
    "            x=torch.exp(x)\n",
    "            s=torch.sum(x)\n",
    "            x=x/s\n",
    "            return x\n",
    "        #\n",
    "        # Print the output for given x at each layer.\n",
    "        #\n",
    "        def output_in_stages(self,x):\n",
    "            print(\"input = \")\n",
    "            print(x)\n",
    "            print(\"\\n\")\n",
    "            ctr=0\n",
    "            for L in self.layers:\n",
    "                x=L(x)\n",
    "                print(\"output \"+str(ctr))\n",
    "                print(x)\n",
    "                print(\"\\n\")\n",
    "                ctr+=1\n",
    "            x=torch.exp(x)\n",
    "            s=torch.sum(x)\n",
    "            x=x/s\n",
    "            print(\"output \"+str(ctr))\n",
    "            print(x)\n",
    "            print(\"\\n\")\n",
    "        \n",
    "    #\n",
    "    # instantiate the network\n",
    "    #\n",
    "    mynetwork=MyNetwork(nfeatures,nclasses,hidden_layer_sizes)\n",
    "    #\n",
    "    # training requires a data loader in which we specify a batch size.\n",
    "    # when we use shuffle=True the rows of the dataset are permuted for each epoch.\n",
    "    #\n",
    "    mydataloader_train=torch.utils.data.DataLoader(dataset=mydata_train, batch_size=batchsize,shuffle=True)\n",
    "    #\n",
    "    # to validate on a test setm we create a data loader for the test set that uses the entire \n",
    "    # test set as a single batch (since we don't train on the test set)\n",
    "    #\n",
    "    mydataloader_test=torch.utils.data.DataLoader(dataset=mydata_test, batch_size=ntest,shuffle=False)\n",
    "\n",
    "    learning_rate = lrate\n",
    "    num_epochs =nepochs\n",
    "\n",
    "    model=mynetwork\n",
    "    #\n",
    "    # move the model to the device\n",
    "    #\n",
    "    model = model.to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    #\n",
    "    # Adam is an optimizer that maintains a different learning \n",
    "    # rate for every weight/parameter in the network - so the learning rate \n",
    "    # is a relative one\n",
    "    #\n",
    "    # See the gentle introduction:\n",
    "    #        e.g. https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/\n",
    "    #\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    #\n",
    "    # Now we are at the training loop\n",
    "    #\n",
    "    # num_epochs = number of times we run throug the entire dataset\n",
    "    #\n",
    "    for epoch in range(num_epochs):\n",
    "        #\n",
    "        # for each epoch the data loader loads the data one batch at a time\n",
    "        #\n",
    "        for i,(x,y) in enumerate(mydataloader_train):\n",
    "            \n",
    "            x=x.float()\n",
    "            x=x.to(device)\n",
    "            y=y.to(device,dtype=torch.long)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs=model(x)\n",
    "            \n",
    "            loss=criterion(outputs,y)\n",
    "            loss_value = loss.item()\n",
    "            \n",
    "            loss.backward() # compute gradients\n",
    "            \n",
    "            # move in direction of minus the gradient by a learning_rate amount \n",
    "            # here because we are using Adam, step is more complicated than -epsilon*Gradient\n",
    "            optimizer.step() \n",
    "            \n",
    "        if epoch==10*int(epoch/10):\n",
    "            print(\"epoch = {0:5d} loss = {1:8.5f}\".format(epoch,loss_value))\n",
    "    print(\"epoch = {0:5d} loss = {1:8.5f}\".format(epoch,loss_value))\n",
    "    #\n",
    "    # Compute the confusion matrix based on test data \n",
    "    # i.e. for each pair of classes c1, c2,\n",
    "    # how often when true class is c1 we predict c2. \n",
    "    #\n",
    "    # Here, for a given test (x,Y) when we calculate the class prediction\n",
    "    # probabilities, we predict the class with the highest prediction probability.\n",
    "    #\n",
    "    # A good confusion matrix should be close to a diagonal matrix.\n",
    "    #\n",
    "    Confusion=np.zeros(shape=(nclasses,nclasses))\n",
    "    for i in range(ntest):\n",
    "        \n",
    "        # get an x value\n",
    "        d=mydata_test[i]\n",
    "        x=torch.tensor(d[0])\n",
    "        x=torch.reshape(x,(1,nfeatures))\n",
    "        x=x.to(device).float()\n",
    "        \n",
    "        # compute the prediction probs for this x\n",
    "        ypred=model(x)\n",
    "        ypred=model(x).cpu().detach().numpy()[0]\n",
    "        \n",
    "        # compute the prediction \n",
    "        # coordinate of ypred taking maximum value\n",
    "        yp=np.argmax(ypred)\n",
    "        \n",
    "        # get the true class\n",
    "        ytrue=int(d[1])\n",
    "        \n",
    "        # update the confusion matrix count\n",
    "        Confusion[ytrue,yp]+=1\n",
    "    #\n",
    "    # accuracy is the proportion of correct predictions\n",
    "    #\n",
    "    accuracy=np.sum(np.diag(Confusion))/np.sum(Confusion)\n",
    "    return(Confusion,accuracy,model)\n",
    "\n",
    "#\n",
    "# use pandas to read the data in as a pandas data frame\n",
    "#\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_csv(\"seeds_dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9ff4216-e5d4-4d61-8b88-abbb3f0cd742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>Perimeter</th>\n",
       "      <th>Compactness</th>\n",
       "      <th>KernelLen</th>\n",
       "      <th>KernelWidth</th>\n",
       "      <th>AsymmetryCoef</th>\n",
       "      <th>KernelGrooveLen</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>10.82</td>\n",
       "      <td>12.83</td>\n",
       "      <td>0.8256</td>\n",
       "      <td>5.180</td>\n",
       "      <td>2.630</td>\n",
       "      <td>4.853</td>\n",
       "      <td>5.089</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>15.57</td>\n",
       "      <td>15.15</td>\n",
       "      <td>0.8527</td>\n",
       "      <td>5.920</td>\n",
       "      <td>3.231</td>\n",
       "      <td>2.640</td>\n",
       "      <td>5.879</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>13.07</td>\n",
       "      <td>13.92</td>\n",
       "      <td>0.8480</td>\n",
       "      <td>5.472</td>\n",
       "      <td>2.994</td>\n",
       "      <td>5.304</td>\n",
       "      <td>5.395</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>11.55</td>\n",
       "      <td>13.10</td>\n",
       "      <td>0.8455</td>\n",
       "      <td>5.167</td>\n",
       "      <td>2.845</td>\n",
       "      <td>6.715</td>\n",
       "      <td>4.956</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>12.74</td>\n",
       "      <td>13.67</td>\n",
       "      <td>0.8564</td>\n",
       "      <td>5.395</td>\n",
       "      <td>2.956</td>\n",
       "      <td>2.504</td>\n",
       "      <td>4.869</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Area  Perimeter  Compactness  KernelLen  KernelWidth  AsymmetryCoef  \\\n",
       "193  10.82      12.83       0.8256      5.180        2.630          4.853   \n",
       "137  15.57      15.15       0.8527      5.920        3.231          2.640   \n",
       "140  13.07      13.92       0.8480      5.472        2.994          5.304   \n",
       "171  11.55      13.10       0.8455      5.167        2.845          6.715   \n",
       "27   12.74      13.67       0.8564      5.395        2.956          2.504   \n",
       "\n",
       "     KernelGrooveLen  Class  \n",
       "193            5.089      3  \n",
       "137            5.879      2  \n",
       "140            5.395      3  \n",
       "171            4.956      3  \n",
       "27             4.869      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# in pandas, shape gives (nrows,ncolums)\n",
    "#\n",
    "N=df.shape[0]\n",
    "I=np.random.permutation(range(N))\n",
    "#\n",
    "# the pandas loc method can be used to reorder the rows\n",
    "#\n",
    "df=df.loc[I]\n",
    "#\n",
    "# print out the result - note that the index stores the original row \n",
    "# number for each observation\n",
    "#\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "293ee6ac-7b86-47bc-a480-21342d49edd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 8)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Convert to numpy arrays and extract the feature matrix X and class column Y\n",
    "# Note that we subtract 1 from class value so that classes are coded 0,1,2 rather than 1,2,3\n",
    "#\n",
    "X=np.array(df)[:,0:7]\n",
    "Y=np.array(df)[:,7]-1 \n",
    "Y=Y.astype(int)\n",
    "print(df.shape)\n",
    "\n",
    "#\n",
    "# Create separate training and testing (X,Y) pairs using 75% train, 25% test\n",
    "# and call these (Xtrain,Ytrain), (Xtest,Ytest)\n",
    "#\n",
    "N=df.shape[0]\n",
    "Ntrain=int(3*N/4)\n",
    "Ntest=N-Ntrain\n",
    "Xtrain=X[0:Ntrain,:]\n",
    "Ytrain=Y[0:Ntrain]\n",
    "Xtest=X[Ntrain:N,:]\n",
    "Ytest=Y[Ntrain:N]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaf5630-522f-458e-8a79-2a763a5665c0",
   "metadata": {},
   "source": [
    "Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ed35292-18bc-42c0-aa83-d93e37eb2389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =     0 loss =  1.11884\n",
      "epoch =    10 loss =  1.00155\n",
      "epoch =    20 loss =  0.83730\n",
      "epoch =    30 loss =  0.62720\n",
      "epoch =    40 loss =  0.71249\n",
      "epoch =    50 loss =  1.01006\n",
      "epoch =    60 loss =  0.48256\n",
      "epoch =    70 loss =  0.58043\n",
      "epoch =    80 loss =  0.56708\n",
      "epoch =    90 loss =  0.26316\n",
      "epoch =   100 loss =  0.44697\n",
      "epoch =   110 loss =  0.16667\n",
      "epoch =   120 loss =  0.26902\n",
      "epoch =   130 loss =  0.15659\n",
      "epoch =   140 loss =  0.08375\n",
      "epoch =   150 loss =  0.04787\n",
      "epoch =   160 loss =  0.06816\n",
      "epoch =   170 loss =  0.19029\n",
      "epoch =   180 loss =  0.07357\n",
      "epoch =   190 loss =  0.08524\n",
      "epoch =   200 loss =  0.19502\n",
      "epoch =   210 loss =  0.06657\n",
      "epoch =   220 loss =  0.14985\n",
      "epoch =   230 loss =  0.44791\n",
      "epoch =   240 loss =  0.15642\n",
      "epoch =   250 loss =  0.71647\n",
      "epoch =   260 loss =  0.04306\n",
      "epoch =   270 loss =  0.05104\n",
      "epoch =   280 loss =  0.08886\n",
      "epoch =   290 loss =  0.12419\n",
      "epoch =   300 loss =  0.01655\n",
      "epoch =   310 loss =  0.11556\n",
      "epoch =   320 loss =  0.18254\n",
      "epoch =   330 loss =  0.02297\n",
      "epoch =   340 loss =  0.27842\n",
      "epoch =   350 loss =  0.01809\n",
      "epoch =   360 loss =  0.03965\n",
      "epoch =   370 loss =  0.18104\n",
      "epoch =   380 loss =  0.35420\n",
      "epoch =   390 loss =  0.06014\n",
      "epoch =   400 loss =  0.11250\n",
      "epoch =   410 loss =  0.06903\n",
      "epoch =   420 loss =  0.02083\n",
      "epoch =   430 loss =  0.40917\n",
      "epoch =   440 loss =  0.05739\n",
      "epoch =   450 loss =  0.32580\n",
      "epoch =   460 loss =  0.06341\n",
      "epoch =   470 loss =  0.46934\n",
      "epoch =   480 loss =  0.33970\n",
      "epoch =   490 loss =  0.57657\n",
      "epoch =   500 loss =  0.00770\n",
      "epoch =   510 loss =  0.23878\n",
      "epoch =   520 loss =  0.02653\n",
      "epoch =   530 loss =  0.01449\n",
      "epoch =   540 loss =  0.09025\n",
      "epoch =   550 loss =  0.15404\n",
      "epoch =   560 loss =  0.10600\n",
      "epoch =   570 loss =  0.06926\n",
      "epoch =   580 loss =  0.02473\n",
      "epoch =   590 loss =  0.34237\n",
      "epoch =   600 loss =  0.35391\n",
      "epoch =   610 loss =  0.05581\n",
      "epoch =   620 loss =  0.45593\n",
      "epoch =   630 loss =  0.49442\n",
      "epoch =   640 loss =  0.09915\n",
      "epoch =   650 loss =  0.01022\n",
      "epoch =   660 loss =  0.17056\n",
      "epoch =   670 loss =  0.05694\n",
      "epoch =   680 loss =  0.01524\n",
      "epoch =   690 loss =  0.23945\n",
      "epoch =   700 loss =  0.05157\n",
      "epoch =   710 loss =  0.16197\n",
      "epoch =   720 loss =  0.05602\n",
      "epoch =   730 loss =  0.23600\n",
      "epoch =   740 loss =  0.51285\n",
      "epoch =   750 loss =  0.03535\n",
      "epoch =   760 loss =  0.04073\n",
      "epoch =   770 loss =  0.18315\n",
      "epoch =   780 loss =  0.01667\n",
      "epoch =   790 loss =  0.07296\n",
      "epoch =   800 loss =  0.50248\n",
      "epoch =   810 loss =  0.03975\n",
      "epoch =   820 loss =  0.03190\n",
      "epoch =   830 loss =  0.04575\n",
      "epoch =   840 loss =  0.02449\n",
      "epoch =   850 loss =  0.13227\n",
      "epoch =   860 loss =  0.02685\n",
      "epoch =   870 loss =  0.01327\n",
      "epoch =   880 loss =  0.03621\n",
      "epoch =   890 loss =  0.14087\n",
      "epoch =   900 loss =  0.13918\n",
      "epoch =   910 loss =  0.37010\n",
      "epoch =   920 loss =  0.07105\n",
      "epoch =   930 loss =  0.47311\n",
      "epoch =   940 loss =  0.07796\n",
      "epoch =   950 loss =  0.10461\n",
      "epoch =   960 loss =  0.12749\n",
      "epoch =   970 loss =  0.02513\n",
      "epoch =   980 loss =  0.20753\n",
      "epoch =   990 loss =  0.02120\n",
      "epoch =   999 loss =  0.01796\n",
      "accuracy on test set = 0.8490566037735849\n",
      "confusion matrix = \n",
      "[[17.  5.  0.]\n",
      " [ 0. 11.  0.]\n",
      " [ 3.  0. 17.]]\n"
     ]
    }
   ],
   "source": [
    "C,acc, model=NeuralNetwork(Xtrain=Xtrain,\n",
    "              Ytrain=Ytrain,\n",
    "              Xtest=Xtest,\n",
    "              Ytest=Ytest,\n",
    "              hidden_layer_sizes=[10,10],\n",
    "              batchsize=50, \n",
    "              nepochs=1000,\n",
    "              lrate=.01)\n",
    "\n",
    "print(\"accuracy on test set = \"+str(acc))\n",
    "print(\"confusion matrix = \")\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a284dd9-b0e9-44c5-b01b-44cf1cfb82dc",
   "metadata": {},
   "source": [
    "Note that we can save the model which has all information about the trained network.\n",
    "I've added a module to the model that computes the output probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28db6425-3a7f-4388-a2c8-001f19828b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.872 0.114 0.014] 0\n",
      "[0.229 0.002 0.769] 2\n",
      "[0.123 0.874 0.003] 1\n",
      "[0.947 0.024 0.03 ] 0\n",
      "[0.012 0.    0.988] 2\n",
      "[0.247 0.001 0.752] 2\n",
      "[0.5   0.002 0.497] 2\n",
      "[0.532 0.    0.468] 2\n",
      "[0.155 0.841 0.004] 1\n",
      "[0.106 0.892 0.002] 1\n",
      "[0.016 0.    0.984] 2\n",
      "[0.875 0.072 0.053] 0\n",
      "[0.05  0.    0.949] 2\n",
      "[0.064 0.936 0.   ] 1\n",
      "[0.069 0.93  0.002] 1\n",
      "[0.09  0.908 0.002] 1\n",
      "[0.859 0.118 0.023] 0\n",
      "[0.05  0.95  0.001] 1\n",
      "[0.842 0.132 0.026] 0\n",
      "[0.008 0.    0.992] 2\n",
      "[0.014 0.985 0.   ] 1\n",
      "[0.057 0.942 0.001] 1\n",
      "[0.402 0.005 0.593] 2\n",
      "[0.911 0.085 0.004] 0\n",
      "[0.068 0.93  0.001] 1\n",
      "[0.909 0.083 0.008] 0\n",
      "[0.092 0.001 0.907] 2\n",
      "[0.627 0.356 0.018] 0\n",
      "[0.106 0.893 0.001] 1\n",
      "[0.069 0.93  0.001] 1\n",
      "[0.408 0.586 0.006] 0\n",
      "[0.249 0.005 0.747] 2\n",
      "[0.962 0.024 0.015] 0\n",
      "[0.056 0.    0.944] 2\n",
      "[0.029 0.971 0.   ] 1\n",
      "[0.743 0.227 0.03 ] 0\n",
      "[0.03 0.97 0.  ] 1\n",
      "[0.165 0.833 0.002] 1\n",
      "[0.037 0.962 0.   ] 1\n",
      "[0.887 0.054 0.058] 0\n",
      "[0.03 0.97 0.  ] 1\n",
      "[0.577 0.419 0.005] 0\n",
      "[0.132 0.867 0.001] 1\n",
      "[0.163 0.835 0.002] 0\n",
      "[0.094 0.    0.906] 2\n",
      "[0.67  0.234 0.096] 0\n",
      "[0.02  0.979 0.   ] 1\n",
      "[0.018 0.    0.982] 2\n",
      "[0.516 0.062 0.422] 2\n",
      "[0.009 0.    0.99 ] 2\n",
      "[0.032 0.968 0.001] 1\n",
      "[0.609 0.035 0.357] 0\n",
      "[0.069 0.93  0.001] 1\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Print output probabilities and true class for each test feature vector\n",
    "#\n",
    "for i in range(Xtest.shape[0]):\n",
    "    x=torch.tensor(Xtest[i,:],dtype=torch.float)\n",
    "    p=model.probability_vector(x).detach().numpy().round(3)\n",
    "    y=Ytest[i]\n",
    "    print(str(p)+\" \" +str(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4540762b-23a2-4a20-8b4d-afb8865d882b",
   "metadata": {},
   "source": [
    "We can also interrogate the model and determine the parameters that define it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75c55c5-1c70-4bba-acd4-5f07c72313c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd618f6-14b4-468f-9506-1f39febfdb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4649cbb-1db6-441b-984c-8a38b224a098",
   "metadata": {},
   "source": [
    "model.named_parameters() gives a generator for getting all of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f076eff0-efda-4034-813f-7cd34666aff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd76511-e773-43d3-9aa2-e973170a5cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    print(param.data)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "25c41e35-a732-4f51-a4ff-4c7fe28c680f",
   "metadata": {},
   "source": [
    "We added a method to the model/network that allows us to see the output in stages.\n",
    "I'd like to make sure I know what's going on by seeing whether I can reproduce these outputs from the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ae08e1-361f-4e07-8508-215f79e4538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.tensor(Xtest[0],dtype=torch.float)\n",
    "model.output_in_stages(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66725589-9f9a-4a6f-84e9-26db1cb7b3eb",
   "metadata": {},
   "source": [
    "Copy all of the parameters into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13d64fb-86f5-4479-9b61-8a32830e057d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LTENSORS=[]\n",
    "p=model.named_parameters()\n",
    "for u,v in p:\n",
    "    LTENSORS.append(v.data)\n",
    "print(LTENSORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91c56e5-5489-48e3-9d8e-b3f42184e4f7",
   "metadata": {},
   "source": [
    "Try to reproduce what the linear transformation does in the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ccc266-0d78-44d9-b782-f2797dc15d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "W=LTENSORS[0]\n",
    "B=LTENSORS[1]\n",
    "x=torch.tensor(Xtest[0],dtype=torch.float)\n",
    "print(W.size())\n",
    "print(B.size())\n",
    "print(x.size())\n",
    "y=torch.matmul(W,x)+B\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efd5bd0-e7ef-4667-a3d2-7a6139a722c7",
   "metadata": {},
   "source": [
    "Check the next tensor in which the sigmoid function is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e1b5fe-d5ed-47c6-a0e4-61d0b5bc99e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "1/(1+torch.exp(-y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2db8a7-141d-4e4c-aaaf-ed5809399d77",
   "metadata": {},
   "source": [
    "Finally, check that when we apply the softmax transformation to the final model output we get the probability vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc52f9ce-0742-4a59-bc60-648181baaff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=model(x)\n",
    "torch.exp(y)/torch.sum(torch.exp(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bd43c0-879d-4841-9e7c-a57b874752a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
