{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3392793-46ec-47ec-b633-d695ab711310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>Perimeter</th>\n",
       "      <th>Compactness</th>\n",
       "      <th>KernelLen</th>\n",
       "      <th>KernelWidth</th>\n",
       "      <th>AsymmetryCoef</th>\n",
       "      <th>KernelGrooveLen</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.26</td>\n",
       "      <td>14.84</td>\n",
       "      <td>0.8710</td>\n",
       "      <td>5.763</td>\n",
       "      <td>3.312</td>\n",
       "      <td>2.221</td>\n",
       "      <td>5.220</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.88</td>\n",
       "      <td>14.57</td>\n",
       "      <td>0.8811</td>\n",
       "      <td>5.554</td>\n",
       "      <td>3.333</td>\n",
       "      <td>1.018</td>\n",
       "      <td>4.956</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.29</td>\n",
       "      <td>14.09</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>5.291</td>\n",
       "      <td>3.337</td>\n",
       "      <td>2.699</td>\n",
       "      <td>4.825</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.84</td>\n",
       "      <td>13.94</td>\n",
       "      <td>0.8955</td>\n",
       "      <td>5.324</td>\n",
       "      <td>3.379</td>\n",
       "      <td>2.259</td>\n",
       "      <td>4.805</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.14</td>\n",
       "      <td>14.99</td>\n",
       "      <td>0.9034</td>\n",
       "      <td>5.658</td>\n",
       "      <td>3.562</td>\n",
       "      <td>1.355</td>\n",
       "      <td>5.175</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Area  Perimeter  Compactness  KernelLen  KernelWidth  AsymmetryCoef  \\\n",
       "0  15.26      14.84       0.8710      5.763        3.312          2.221   \n",
       "1  14.88      14.57       0.8811      5.554        3.333          1.018   \n",
       "2  14.29      14.09       0.9050      5.291        3.337          2.699   \n",
       "3  13.84      13.94       0.8955      5.324        3.379          2.259   \n",
       "4  16.14      14.99       0.9034      5.658        3.562          1.355   \n",
       "\n",
       "   KernelGrooveLen  Class  \n",
       "0            5.220      1  \n",
       "1            4.956      1  \n",
       "2            4.825      1  \n",
       "3            4.805      1  \n",
       "4            5.175      1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Compute neural network classification rate.\n",
    "#\n",
    "#   Inputs: \n",
    "#      Xtrain = training feature data\n",
    "#               numpy array rows are obs and columns are features\n",
    "#      Ytrain = vector of coded classes coded 0,1,...,nclasses-1     \n",
    "#      Xtest = test feature data\n",
    "#      Ytest =  vector of coded classes (labels need to be 0,1,...,nclasses-1)\n",
    "#      hidden_layer_sizes = list with numbers of nodes in hidden layers\n",
    "#      batchsize = number of obs in each batch\n",
    "#      nepochs = number of epochs to train on\n",
    "#      lrate = learning rate to use to train\n",
    "#\n",
    "#\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "#\n",
    "# This function takes as input the following:\n",
    "#\n",
    "#   Xtrain = training data predictor variables\n",
    "#            a np 2d array with rows=observations and columns=variables\n",
    "#   Ytrain = training data response variable a np array 1d array\n",
    "#   Xtest = test predictor vars\n",
    "#   Ytest = test response var\n",
    "#   hidden_layer_sizes = list with the number of nodes at intermediate layers\n",
    "#   batchsize = observations are broken up into batches so this is number of obs/batch\n",
    "#   nepochs = when fitting, we go through the entire dataset nepochs times\n",
    "#   lrate = parameter that determine stepsize (this is a bit complicated to describe for the \n",
    "#           optimizer employed (Adam)\n",
    "#\n",
    "def NeuralNetwork(Xtrain,Ytrain,Xtest,Ytest,hidden_layer_sizes,batchsize, nepochs,lrate):\n",
    "    nobs,nfeatures=Xtrain.shape\n",
    "    ntest=Xtest.shape[0]\n",
    "    nclasses=len(set(Ytrain))\n",
    "    \n",
    "    #device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device=\"cpu\" # don't use gpu - when I run this in jupyter I get an error that does not come up \n",
    "    \n",
    "    #\n",
    "    # Dataset is a torch class - we **inherit** from this class to make our own \n",
    "    # custom MyDataset objects. The algorithm for fitting a neural network to the \n",
    "    # data needs a couple of functions.\n",
    "    #\n",
    "    # 1) __len__() giving the number of observations in the dataset\n",
    "    # 2) __getitem__() - enabling one to get a pair/row of X,Y values by using square brackets\n",
    "    # \n",
    "    class MyDataSet(Dataset):\n",
    "        def __init__(self,X,Y):\n",
    "            self.X=X\n",
    "            self.Y=Y\n",
    "            self.N=self.X.shape[0]\n",
    "            self.K=self.X.shape[1]\n",
    "        def __len__(self):\n",
    "            return(self.N)\n",
    "        def __getitem__(self, idx):\n",
    "            return self.X[idx],self.Y[idx]\n",
    "    #\n",
    "    # Here we construct two Dataset objects\n",
    "    #\n",
    "    mydata_train=MyDataSet(Xtrain,Ytrain)\n",
    "    mydata_test=MyDataSet(Xtest,Ytest)\n",
    "    #\n",
    "    # Create a network inheriting from torch.nn.Module\n",
    "    # I've introduced my own constructor.\n",
    "    #\n",
    "    class MyNetwork(torch.nn.Module):\n",
    "        #\n",
    "        # the constructor takes as arguments:\n",
    "        # \n",
    "        #    number of features\n",
    "        #    number of classes\n",
    "        #    a list of sizes of numbers of nodes in the hidden layers\n",
    "        #\n",
    "        def __init__(self,nfeatures,nclasses, hiddenlayersizes):\n",
    "            nhiddenlayers=len(hiddenlayersizes)\n",
    "            #\n",
    "            # the following is used since the constructor for the base class\n",
    "            # was over-ridden, but we still need some attributes that\n",
    "            # are contained in the super class (torch.nn.Module)\n",
    "            #\n",
    "            super().__init__()\n",
    "            #\n",
    "            # create the layers\n",
    "            #\n",
    "            if nhiddenlayers==0:\n",
    "                layers=[]\n",
    "                layers.append(nn.Linear(nfeatures,nclasses))\n",
    "            else:\n",
    "                layers=[]\n",
    "                layers.append(nn.Linear(nfeatures,hiddenlayersizes[0])) # fully connected\n",
    "                layers.append(nn.Sigmoid()) # activation function \n",
    "                for i in range(1,nhiddenlayers):\n",
    "                    layers.append(nn.Linear(hiddenlayersizes[i-1],hiddenlayersizes[i]))\n",
    "                    layers.append(nn.Sigmoid()) # activation\n",
    "                layers.append(nn.Linear(hiddenlayersizes[nhiddenlayers-1],nclasses))\n",
    "                #\n",
    "                # note that when using cross-entropy loss function \n",
    "                # we don't need to include an activation function after the last linear\n",
    "                # transformation\n",
    "                #\n",
    "                # cross entropy loss calculates the output to exp(output)/sum(exp(output))\n",
    "                # to convert to probability vector and compares with true class value\n",
    "                #\n",
    "                self.layers=nn.ModuleList(layers)\n",
    "        #\n",
    "        # Calculates the output from the input feature vector\n",
    "        # This function is needed when model() is called.\n",
    "        #\n",
    "        def forward(self, x):\n",
    "            for L in self.layers:\n",
    "                x=L(x)\n",
    "            return x\n",
    "        #\n",
    "        # Function to compute output probabilities from input.\n",
    "        # This should be the same as y=model(x) followed by softmax\n",
    "        # i.e. exp(y)/sum(exp(y))\n",
    "        #\n",
    "        def probability_vector(self,x):\n",
    "            for L in self.layers:\n",
    "                x=L(x)\n",
    "            x=torch.exp(x)\n",
    "            s=torch.sum(x)\n",
    "            x=x/s\n",
    "            return x\n",
    "        #\n",
    "        # Print the output for given x at each layer.\n",
    "        #\n",
    "        def output_in_stages(self,x):\n",
    "            print(\"input = \")\n",
    "            print(x)\n",
    "            print(\"\\n\")\n",
    "            ctr=0\n",
    "            for L in self.layers:\n",
    "                x=L(x)\n",
    "                print(\"output \"+str(ctr))\n",
    "                print(x)\n",
    "                print(\"\\n\")\n",
    "                ctr+=1\n",
    "            x=torch.exp(x)\n",
    "            s=torch.sum(x)\n",
    "            x=x/s\n",
    "            print(\"output \"+str(ctr))\n",
    "            print(x)\n",
    "            print(\"\\n\")\n",
    "        \n",
    "    #\n",
    "    # instantiate the network\n",
    "    #\n",
    "    mynetwork=MyNetwork(nfeatures,nclasses,hidden_layer_sizes)\n",
    "    #\n",
    "    # training requires a data loader in which we specify a batch size.\n",
    "    # when we use shuffle=True the rows of the dataset are permuted for each epoch.\n",
    "    #\n",
    "    mydataloader_train=torch.utils.data.DataLoader(dataset=mydata_train, batch_size=batchsize,shuffle=True)\n",
    "    #\n",
    "    # to validate on a test setm we create a data loader for the test set that uses the entire \n",
    "    # test set as a single batch (since we don't train on the test set)\n",
    "    #\n",
    "    mydataloader_test=torch.utils.data.DataLoader(dataset=mydata_test, batch_size=ntest,shuffle=False)\n",
    "\n",
    "    learning_rate = lrate\n",
    "    num_epochs =nepochs\n",
    "\n",
    "    model=mynetwork\n",
    "    #\n",
    "    # move the model to the device\n",
    "    #\n",
    "    model = model.to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    #\n",
    "    # Adam is an optimizer that maintains a different learning \n",
    "    # rate for every weight/parameter in the network - so the learning rate \n",
    "    # is a relative one\n",
    "    #\n",
    "    # See the gentle introduction:\n",
    "    #        e.g. https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/\n",
    "    #\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    #\n",
    "    # Now we are at the training loop\n",
    "    #\n",
    "    # num_epochs = number of times we run throug the entire dataset\n",
    "    #\n",
    "    for epoch in range(num_epochs):\n",
    "        #\n",
    "        # for each epoch the data loader loads the data one batch at a time\n",
    "        #\n",
    "        for i,(x,y) in enumerate(mydataloader_train):\n",
    "            \n",
    "            x=x.float()\n",
    "            x=x.to(device)\n",
    "            y=y.to(device,dtype=torch.long)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs=model(x)\n",
    "            \n",
    "            loss=criterion(outputs,y)\n",
    "            loss_value = loss.item()\n",
    "            \n",
    "            loss.backward() # compute gradients\n",
    "            \n",
    "            # move in direction of minus the gradient by a learning_rate amount \n",
    "            # here because we are using Adam, step is more complicated than -epsilon*Gradient\n",
    "            optimizer.step() \n",
    "            \n",
    "        if epoch==10*int(epoch/10):\n",
    "            print(\"epoch = {0:5d} loss = {1:8.5f}\".format(epoch,loss_value))\n",
    "    print(\"epoch = {0:5d} loss = {1:8.5f}\".format(epoch,loss_value))\n",
    "    #\n",
    "    # Compute the confusion matrix based on test data \n",
    "    # i.e. for each pair of classes c1, c2,\n",
    "    # how often when true class is c1 we predict c2. \n",
    "    #\n",
    "    # Here, for a given test (x,Y) when we calculate the class prediction\n",
    "    # probabilities, we predict the class with the highest prediction probability.\n",
    "    #\n",
    "    # A good confusion matrix should be close to a diagonal matrix.\n",
    "    #\n",
    "    Confusion=np.zeros(shape=(nclasses,nclasses))\n",
    "    for i in range(ntest):\n",
    "        \n",
    "        # get an x value\n",
    "        d=mydata_test[i]\n",
    "        x=torch.tensor(d[0])\n",
    "        x=torch.reshape(x,(1,nfeatures))\n",
    "        x=x.to(device).float()\n",
    "        \n",
    "        # compute the prediction probs for this x\n",
    "        ypred=model(x)\n",
    "        ypred=model(x).cpu().detach().numpy()[0]\n",
    "        \n",
    "        # compute the prediction \n",
    "        # coordinate of ypred taking maximum value\n",
    "        yp=np.argmax(ypred)\n",
    "        \n",
    "        # get the true class\n",
    "        ytrue=int(d[1])\n",
    "        \n",
    "        # update the confusion matrix count\n",
    "        Confusion[ytrue,yp]+=1\n",
    "    #\n",
    "    # accuracy is the proportion of correct predictions\n",
    "    #\n",
    "    accuracy=np.sum(np.diag(Confusion))/np.sum(Confusion)\n",
    "    return(Confusion,accuracy,model)\n",
    "\n",
    "#\n",
    "# use pandas to read the data in as a pandas data frame\n",
    "#\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_csv(\"seeds_dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9ff4216-e5d4-4d61-8b88-abbb3f0cd742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>Perimeter</th>\n",
       "      <th>Compactness</th>\n",
       "      <th>KernelLen</th>\n",
       "      <th>KernelWidth</th>\n",
       "      <th>AsymmetryCoef</th>\n",
       "      <th>KernelGrooveLen</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>19.31</td>\n",
       "      <td>16.59</td>\n",
       "      <td>0.8815</td>\n",
       "      <td>6.341</td>\n",
       "      <td>3.810</td>\n",
       "      <td>3.477</td>\n",
       "      <td>6.238</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>14.09</td>\n",
       "      <td>14.41</td>\n",
       "      <td>0.8529</td>\n",
       "      <td>5.717</td>\n",
       "      <td>3.186</td>\n",
       "      <td>3.920</td>\n",
       "      <td>5.299</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>11.87</td>\n",
       "      <td>13.02</td>\n",
       "      <td>0.8795</td>\n",
       "      <td>5.132</td>\n",
       "      <td>2.953</td>\n",
       "      <td>3.597</td>\n",
       "      <td>5.132</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>10.74</td>\n",
       "      <td>12.73</td>\n",
       "      <td>0.8329</td>\n",
       "      <td>5.145</td>\n",
       "      <td>2.642</td>\n",
       "      <td>4.702</td>\n",
       "      <td>4.963</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>12.62</td>\n",
       "      <td>13.67</td>\n",
       "      <td>0.8481</td>\n",
       "      <td>5.410</td>\n",
       "      <td>2.911</td>\n",
       "      <td>3.306</td>\n",
       "      <td>5.231</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Area  Perimeter  Compactness  KernelLen  KernelWidth  AsymmetryCoef  \\\n",
       "96   19.31      16.59       0.8815      6.341        3.810          3.477   \n",
       "32   14.09      14.41       0.8529      5.717        3.186          3.920   \n",
       "192  11.87      13.02       0.8795      5.132        2.953          3.597   \n",
       "177  10.74      12.73       0.8329      5.145        2.642          4.702   \n",
       "198  12.62      13.67       0.8481      5.410        2.911          3.306   \n",
       "\n",
       "     KernelGrooveLen  Class  \n",
       "96             6.238      2  \n",
       "32             5.299      1  \n",
       "192            5.132      3  \n",
       "177            4.963      3  \n",
       "198            5.231      3  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# in pandas, shape gives (nrows,ncolums)\n",
    "#\n",
    "N=df.shape[0]\n",
    "I=np.random.permutation(range(N))\n",
    "#\n",
    "# the pandas loc method can be used to reorder the rows\n",
    "#\n",
    "df=df.loc[I]\n",
    "#\n",
    "# print out the result - note that the index stores the original row \n",
    "# number for each observation\n",
    "#\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "293ee6ac-7b86-47bc-a480-21342d49edd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 8)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Convert to numpy arrays and extract the feature matrix X and class column Y\n",
    "# Note that we subtract 1 from class value so that classes are coded 0,1,2 rather than 1,2,3\n",
    "#\n",
    "X=np.array(df)[:,0:7]\n",
    "Y=np.array(df)[:,7]-1 \n",
    "Y=Y.astype(int)\n",
    "print(df.shape)\n",
    "\n",
    "#\n",
    "# Create separate training and testing (X,Y) pairs using 75% train, 25% test\n",
    "# and call these (Xtrain,Ytrain), (Xtest,Ytest)\n",
    "#\n",
    "N=df.shape[0]\n",
    "Ntrain=int(2*N/4)\n",
    "Ntest=N-Ntrain\n",
    "Xtrain=X[0:Ntrain,:]\n",
    "Ytrain=Y[0:Ntrain]\n",
    "Xtest=X[Ntrain:N,:]\n",
    "Ytest=Y[Ntrain:N]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaf5630-522f-458e-8a79-2a763a5665c0",
   "metadata": {},
   "source": [
    "Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ed35292-18bc-42c0-aa83-d93e37eb2389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =     0 loss =  1.15833\n",
      "epoch =    10 loss =  1.10831\n",
      "epoch =    20 loss =  1.11373\n",
      "epoch =    30 loss =  1.00448\n",
      "epoch =    40 loss =  1.07995\n",
      "epoch =    50 loss =  1.01043\n",
      "epoch =    60 loss =  1.03605\n",
      "epoch =    70 loss =  1.03284\n",
      "epoch =    80 loss =  0.99429\n",
      "epoch =    90 loss =  1.01354\n",
      "epoch =   100 loss =  1.04458\n",
      "epoch =   110 loss =  0.92329\n",
      "epoch =   120 loss =  1.04202\n",
      "epoch =   130 loss =  0.89808\n",
      "epoch =   140 loss =  0.98956\n",
      "epoch =   150 loss =  0.89309\n",
      "epoch =   160 loss =  0.82420\n",
      "epoch =   170 loss =  0.92643\n",
      "epoch =   180 loss =  0.97465\n",
      "epoch =   190 loss =  1.05567\n",
      "epoch =   200 loss =  0.77150\n",
      "epoch =   210 loss =  0.97265\n",
      "epoch =   220 loss =  0.71918\n",
      "epoch =   230 loss =  0.71299\n",
      "epoch =   240 loss =  0.64919\n",
      "epoch =   250 loss =  0.74791\n",
      "epoch =   260 loss =  0.60894\n",
      "epoch =   270 loss =  0.51250\n",
      "epoch =   280 loss =  0.63156\n",
      "epoch =   290 loss =  0.68476\n",
      "epoch =   300 loss =  0.85878\n",
      "epoch =   310 loss =  0.76290\n",
      "epoch =   320 loss =  0.64113\n",
      "epoch =   330 loss =  0.61801\n",
      "epoch =   340 loss =  0.54672\n",
      "epoch =   350 loss =  0.52032\n",
      "epoch =   360 loss =  0.75480\n",
      "epoch =   370 loss =  0.60050\n",
      "epoch =   380 loss =  0.55523\n",
      "epoch =   390 loss =  0.66303\n",
      "epoch =   400 loss =  0.54521\n",
      "epoch =   410 loss =  0.72322\n",
      "epoch =   420 loss =  0.73746\n",
      "epoch =   430 loss =  0.59925\n",
      "epoch =   440 loss =  0.61170\n",
      "epoch =   450 loss =  0.44057\n",
      "epoch =   460 loss =  0.56658\n",
      "epoch =   470 loss =  0.39552\n",
      "epoch =   480 loss =  0.73879\n",
      "epoch =   490 loss =  0.39438\n",
      "epoch =   500 loss =  0.54287\n",
      "epoch =   510 loss =  0.57380\n",
      "epoch =   520 loss =  0.80434\n",
      "epoch =   530 loss =  0.47413\n",
      "epoch =   540 loss =  0.36062\n",
      "epoch =   550 loss =  0.51263\n",
      "epoch =   560 loss =  0.40033\n",
      "epoch =   570 loss =  0.54250\n",
      "epoch =   580 loss =  0.35763\n",
      "epoch =   590 loss =  0.26336\n",
      "epoch =   600 loss =  0.77036\n",
      "epoch =   610 loss =  0.36871\n",
      "epoch =   620 loss =  0.47771\n",
      "epoch =   630 loss =  0.34719\n",
      "epoch =   640 loss =  0.35654\n",
      "epoch =   650 loss =  0.48995\n",
      "epoch =   660 loss =  0.27522\n",
      "epoch =   670 loss =  0.51939\n",
      "epoch =   680 loss =  0.27191\n",
      "epoch =   690 loss =  0.37429\n",
      "epoch =   700 loss =  0.42962\n",
      "epoch =   710 loss =  0.47586\n",
      "epoch =   720 loss =  0.50985\n",
      "epoch =   730 loss =  0.33479\n",
      "epoch =   740 loss =  0.29301\n",
      "epoch =   750 loss =  0.30190\n",
      "epoch =   760 loss =  0.32795\n",
      "epoch =   770 loss =  0.49538\n",
      "epoch =   780 loss =  0.50923\n",
      "epoch =   790 loss =  0.48152\n",
      "epoch =   800 loss =  0.20530\n",
      "epoch =   810 loss =  0.29270\n",
      "epoch =   820 loss =  0.23929\n",
      "epoch =   830 loss =  0.43231\n",
      "epoch =   840 loss =  0.45623\n",
      "epoch =   850 loss =  0.27904\n",
      "epoch =   860 loss =  0.18156\n",
      "epoch =   870 loss =  0.41237\n",
      "epoch =   880 loss =  0.38375\n",
      "epoch =   890 loss =  0.17373\n",
      "epoch =   900 loss =  0.31141\n",
      "epoch =   910 loss =  0.13856\n",
      "epoch =   920 loss =  0.26006\n",
      "epoch =   930 loss =  0.37874\n",
      "epoch =   940 loss =  0.66634\n",
      "epoch =   950 loss =  0.37738\n",
      "epoch =   960 loss =  0.23677\n",
      "epoch =   970 loss =  0.43540\n",
      "epoch =   980 loss =  0.17409\n",
      "epoch =   990 loss =  0.39539\n",
      "epoch =  1000 loss =  0.08859\n",
      "epoch =  1010 loss =  0.20811\n",
      "epoch =  1020 loss =  0.33919\n",
      "epoch =  1030 loss =  0.09342\n",
      "epoch =  1040 loss =  0.23792\n",
      "epoch =  1050 loss =  0.32584\n",
      "epoch =  1060 loss =  0.12858\n",
      "epoch =  1070 loss =  0.21818\n",
      "epoch =  1080 loss =  0.14457\n",
      "epoch =  1090 loss =  0.39119\n",
      "epoch =  1100 loss =  0.10658\n",
      "epoch =  1110 loss =  0.16757\n",
      "epoch =  1120 loss =  0.06313\n",
      "epoch =  1130 loss =  0.07148\n",
      "epoch =  1140 loss =  0.32770\n",
      "epoch =  1150 loss =  0.21481\n",
      "epoch =  1160 loss =  0.38283\n",
      "epoch =  1170 loss =  0.27983\n",
      "epoch =  1180 loss =  0.06556\n",
      "epoch =  1190 loss =  0.20029\n",
      "epoch =  1200 loss =  0.12849\n",
      "epoch =  1210 loss =  0.19093\n",
      "epoch =  1220 loss =  0.21101\n",
      "epoch =  1230 loss =  0.19802\n",
      "epoch =  1240 loss =  0.23800\n",
      "epoch =  1250 loss =  0.13058\n",
      "epoch =  1260 loss =  0.10650\n",
      "epoch =  1270 loss =  0.47625\n",
      "epoch =  1280 loss =  0.17068\n",
      "epoch =  1290 loss =  0.06813\n",
      "epoch =  1300 loss =  0.11151\n",
      "epoch =  1310 loss =  0.10183\n",
      "epoch =  1320 loss =  0.20531\n",
      "epoch =  1330 loss =  0.27708\n",
      "epoch =  1340 loss =  0.05686\n",
      "epoch =  1350 loss =  0.07450\n",
      "epoch =  1360 loss =  0.19136\n",
      "epoch =  1370 loss =  0.07466\n",
      "epoch =  1380 loss =  0.11934\n",
      "epoch =  1390 loss =  0.24421\n",
      "epoch =  1400 loss =  0.09259\n",
      "epoch =  1410 loss =  0.05223\n",
      "epoch =  1420 loss =  0.13513\n",
      "epoch =  1430 loss =  0.25802\n",
      "epoch =  1440 loss =  0.18969\n",
      "epoch =  1450 loss =  0.13198\n",
      "epoch =  1460 loss =  0.41502\n",
      "epoch =  1470 loss =  0.06189\n",
      "epoch =  1480 loss =  0.10157\n",
      "epoch =  1490 loss =  0.23495\n",
      "epoch =  1500 loss =  0.16815\n",
      "epoch =  1510 loss =  0.06486\n",
      "epoch =  1520 loss =  0.23267\n",
      "epoch =  1530 loss =  0.05931\n",
      "epoch =  1540 loss =  0.09190\n",
      "epoch =  1550 loss =  0.51825\n",
      "epoch =  1560 loss =  0.33870\n",
      "epoch =  1570 loss =  0.04428\n",
      "epoch =  1580 loss =  0.11466\n",
      "epoch =  1590 loss =  0.08378\n",
      "epoch =  1600 loss =  0.07590\n",
      "epoch =  1610 loss =  0.08142\n",
      "epoch =  1620 loss =  0.07763\n",
      "epoch =  1630 loss =  0.04800\n",
      "epoch =  1640 loss =  0.05569\n",
      "epoch =  1650 loss =  0.56661\n",
      "epoch =  1660 loss =  0.05210\n",
      "epoch =  1670 loss =  0.28085\n",
      "epoch =  1680 loss =  0.25217\n",
      "epoch =  1690 loss =  0.13883\n",
      "epoch =  1700 loss =  0.03182\n",
      "epoch =  1710 loss =  0.09456\n",
      "epoch =  1720 loss =  0.20646\n",
      "epoch =  1730 loss =  0.15397\n",
      "epoch =  1740 loss =  0.29805\n",
      "epoch =  1750 loss =  0.12749\n",
      "epoch =  1760 loss =  0.02749\n",
      "epoch =  1770 loss =  0.20384\n",
      "epoch =  1780 loss =  0.30301\n",
      "epoch =  1790 loss =  0.05918\n",
      "epoch =  1800 loss =  0.16849\n",
      "epoch =  1810 loss =  0.06497\n",
      "epoch =  1820 loss =  0.22774\n",
      "epoch =  1830 loss =  0.02631\n",
      "epoch =  1840 loss =  0.05801\n",
      "epoch =  1850 loss =  0.19426\n",
      "epoch =  1860 loss =  0.05215\n",
      "epoch =  1870 loss =  0.06438\n",
      "epoch =  1880 loss =  0.26882\n",
      "epoch =  1890 loss =  0.41481\n",
      "epoch =  1900 loss =  0.07148\n",
      "epoch =  1910 loss =  0.24830\n",
      "epoch =  1920 loss =  0.03056\n",
      "epoch =  1930 loss =  0.14513\n",
      "epoch =  1940 loss =  0.21791\n",
      "epoch =  1950 loss =  0.23084\n",
      "epoch =  1960 loss =  0.04374\n",
      "epoch =  1970 loss =  0.20568\n",
      "epoch =  1980 loss =  0.05505\n",
      "epoch =  1990 loss =  0.23715\n",
      "epoch =  2000 loss =  0.21137\n",
      "epoch =  2010 loss =  0.12977\n",
      "epoch =  2020 loss =  0.05719\n",
      "epoch =  2030 loss =  0.16123\n",
      "epoch =  2040 loss =  0.30014\n",
      "epoch =  2050 loss =  0.03776\n",
      "epoch =  2060 loss =  0.14652\n",
      "epoch =  2070 loss =  0.10962\n",
      "epoch =  2080 loss =  0.09210\n",
      "epoch =  2090 loss =  0.13187\n",
      "epoch =  2100 loss =  0.02607\n",
      "epoch =  2110 loss =  0.22671\n",
      "epoch =  2120 loss =  0.34589\n",
      "epoch =  2130 loss =  0.16865\n",
      "epoch =  2140 loss =  0.34297\n",
      "epoch =  2150 loss =  0.41282\n",
      "epoch =  2160 loss =  0.72113\n",
      "epoch =  2170 loss =  0.11908\n",
      "epoch =  2180 loss =  0.29944\n",
      "epoch =  2190 loss =  0.06167\n",
      "epoch =  2200 loss =  0.15972\n",
      "epoch =  2210 loss =  0.05444\n",
      "epoch =  2220 loss =  0.05321\n",
      "epoch =  2230 loss =  0.50464\n",
      "epoch =  2240 loss =  0.33930\n",
      "epoch =  2250 loss =  0.05698\n",
      "epoch =  2260 loss =  0.16111\n",
      "epoch =  2270 loss =  0.04283\n",
      "epoch =  2280 loss =  0.03640\n",
      "epoch =  2290 loss =  0.56343\n",
      "epoch =  2300 loss =  0.08596\n",
      "epoch =  2310 loss =  0.33963\n",
      "epoch =  2320 loss =  0.47067\n",
      "epoch =  2330 loss =  0.32611\n",
      "epoch =  2340 loss =  0.04910\n",
      "epoch =  2350 loss =  0.40889\n",
      "epoch =  2360 loss =  0.04557\n",
      "epoch =  2370 loss =  0.19141\n",
      "epoch =  2380 loss =  0.02872\n",
      "epoch =  2390 loss =  0.18509\n",
      "epoch =  2400 loss =  0.14058\n",
      "epoch =  2410 loss =  0.57540\n",
      "epoch =  2420 loss =  0.45842\n",
      "epoch =  2430 loss =  0.09050\n",
      "epoch =  2440 loss =  0.36731\n",
      "epoch =  2450 loss =  0.03427\n",
      "epoch =  2460 loss =  0.22961\n",
      "epoch =  2470 loss =  0.14628\n",
      "epoch =  2480 loss =  0.03138\n",
      "epoch =  2490 loss =  0.01353\n",
      "epoch =  2500 loss =  0.17735\n",
      "epoch =  2510 loss =  0.22108\n",
      "epoch =  2520 loss =  0.19545\n",
      "epoch =  2530 loss =  0.23185\n",
      "epoch =  2540 loss =  0.02331\n",
      "epoch =  2550 loss =  0.36944\n",
      "epoch =  2560 loss =  0.02544\n",
      "epoch =  2570 loss =  0.06011\n",
      "epoch =  2580 loss =  0.01537\n",
      "epoch =  2590 loss =  0.03671\n",
      "epoch =  2600 loss =  0.02541\n",
      "epoch =  2610 loss =  0.20759\n",
      "epoch =  2620 loss =  0.05094\n",
      "epoch =  2630 loss =  0.15827\n",
      "epoch =  2640 loss =  0.19722\n",
      "epoch =  2650 loss =  0.09264\n",
      "epoch =  2660 loss =  0.26362\n",
      "epoch =  2670 loss =  0.01499\n",
      "epoch =  2680 loss =  0.13587\n",
      "epoch =  2690 loss =  0.02836\n",
      "epoch =  2700 loss =  0.29436\n",
      "epoch =  2710 loss =  0.01720\n",
      "epoch =  2720 loss =  0.08188\n",
      "epoch =  2730 loss =  0.29861\n",
      "epoch =  2740 loss =  0.21179\n",
      "epoch =  2750 loss =  0.32213\n",
      "epoch =  2760 loss =  0.07167\n",
      "epoch =  2770 loss =  0.05474\n",
      "epoch =  2780 loss =  0.36968\n",
      "epoch =  2790 loss =  0.26079\n",
      "epoch =  2800 loss =  0.46978\n",
      "epoch =  2810 loss =  0.26568\n",
      "epoch =  2820 loss =  0.01565\n",
      "epoch =  2830 loss =  0.02943\n",
      "epoch =  2840 loss =  0.18514\n",
      "epoch =  2850 loss =  0.07121\n",
      "epoch =  2860 loss =  0.03158\n",
      "epoch =  2870 loss =  0.32615\n",
      "epoch =  2880 loss =  0.55074\n",
      "epoch =  2890 loss =  0.01869\n",
      "epoch =  2900 loss =  0.02915\n",
      "epoch =  2910 loss =  0.14427\n",
      "epoch =  2920 loss =  0.41529\n",
      "epoch =  2930 loss =  0.09927\n",
      "epoch =  2940 loss =  0.12185\n",
      "epoch =  2950 loss =  0.02932\n",
      "epoch =  2960 loss =  0.17167\n",
      "epoch =  2970 loss =  0.07200\n",
      "epoch =  2980 loss =  0.12775\n",
      "epoch =  2990 loss =  0.06443\n",
      "epoch =  3000 loss =  0.03901\n",
      "epoch =  3010 loss =  0.22287\n",
      "epoch =  3020 loss =  0.04775\n",
      "epoch =  3030 loss =  0.27175\n",
      "epoch =  3040 loss =  0.59589\n",
      "epoch =  3050 loss =  0.02794\n",
      "epoch =  3060 loss =  0.11318\n",
      "epoch =  3070 loss =  0.11008\n",
      "epoch =  3080 loss =  0.13035\n",
      "epoch =  3090 loss =  0.02662\n",
      "epoch =  3100 loss =  0.14703\n",
      "epoch =  3110 loss =  0.02470\n",
      "epoch =  3120 loss =  0.04177\n",
      "epoch =  3130 loss =  0.06131\n",
      "epoch =  3140 loss =  0.03994\n",
      "epoch =  3150 loss =  0.71332\n",
      "epoch =  3160 loss =  0.03746\n",
      "epoch =  3170 loss =  0.24560\n",
      "epoch =  3180 loss =  0.01380\n",
      "epoch =  3190 loss =  0.01489\n",
      "epoch =  3200 loss =  0.04948\n",
      "epoch =  3210 loss =  0.08461\n",
      "epoch =  3220 loss =  0.13742\n",
      "epoch =  3230 loss =  0.04012\n",
      "epoch =  3240 loss =  0.01389\n",
      "epoch =  3250 loss =  0.03447\n",
      "epoch =  3260 loss =  0.05602\n",
      "epoch =  3270 loss =  0.60813\n",
      "epoch =  3280 loss =  0.18208\n",
      "epoch =  3290 loss =  0.04781\n",
      "epoch =  3300 loss =  0.03793\n",
      "epoch =  3310 loss =  0.02127\n",
      "epoch =  3320 loss =  0.75839\n",
      "epoch =  3330 loss =  0.03901\n",
      "epoch =  3340 loss =  0.20060\n",
      "epoch =  3350 loss =  0.03275\n",
      "epoch =  3360 loss =  0.30474\n",
      "epoch =  3370 loss =  0.14950\n",
      "epoch =  3380 loss =  0.21328\n",
      "epoch =  3390 loss =  0.01646\n",
      "epoch =  3400 loss =  0.20050\n",
      "epoch =  3410 loss =  0.02358\n",
      "epoch =  3420 loss =  0.19441\n",
      "epoch =  3430 loss =  0.44171\n",
      "epoch =  3440 loss =  0.05050\n",
      "epoch =  3450 loss =  0.08731\n",
      "epoch =  3460 loss =  0.08146\n",
      "epoch =  3470 loss =  0.38523\n",
      "epoch =  3480 loss =  0.27817\n",
      "epoch =  3490 loss =  0.32699\n",
      "epoch =  3500 loss =  0.02110\n",
      "epoch =  3510 loss =  0.28372\n",
      "epoch =  3520 loss =  0.01161\n",
      "epoch =  3530 loss =  0.33988\n",
      "epoch =  3540 loss =  0.26771\n",
      "epoch =  3550 loss =  0.03016\n",
      "epoch =  3560 loss =  0.68863\n",
      "epoch =  3570 loss =  0.04497\n",
      "epoch =  3580 loss =  0.21709\n",
      "epoch =  3590 loss =  0.09233\n",
      "epoch =  3600 loss =  0.20269\n",
      "epoch =  3610 loss =  0.03725\n",
      "epoch =  3620 loss =  0.05550\n",
      "epoch =  3630 loss =  0.21547\n",
      "epoch =  3640 loss =  0.01354\n",
      "epoch =  3650 loss =  0.00576\n",
      "epoch =  3660 loss =  0.02346\n",
      "epoch =  3670 loss =  0.08627\n",
      "epoch =  3680 loss =  0.08856\n",
      "epoch =  3690 loss =  0.12253\n",
      "epoch =  3700 loss =  0.09553\n",
      "epoch =  3710 loss =  0.19247\n",
      "epoch =  3720 loss =  0.35956\n",
      "epoch =  3730 loss =  0.02375\n",
      "epoch =  3740 loss =  0.04608\n",
      "epoch =  3750 loss =  0.15249\n",
      "epoch =  3760 loss =  0.03921\n",
      "epoch =  3770 loss =  0.02254\n",
      "epoch =  3780 loss =  0.01987\n",
      "epoch =  3790 loss =  0.18410\n",
      "epoch =  3800 loss =  0.16259\n",
      "epoch =  3810 loss =  0.39586\n",
      "epoch =  3820 loss =  0.04647\n",
      "epoch =  3830 loss =  0.02870\n",
      "epoch =  3840 loss =  0.04010\n",
      "epoch =  3850 loss =  0.11119\n",
      "epoch =  3860 loss =  0.65844\n",
      "epoch =  3870 loss =  0.03594\n",
      "epoch =  3880 loss =  0.17681\n",
      "epoch =  3890 loss =  0.65394\n",
      "epoch =  3900 loss =  0.01224\n",
      "epoch =  3910 loss =  0.01360\n",
      "epoch =  3920 loss =  0.24295\n",
      "epoch =  3930 loss =  0.04180\n",
      "epoch =  3940 loss =  0.23773\n",
      "epoch =  3950 loss =  0.05468\n",
      "epoch =  3960 loss =  0.13447\n",
      "epoch =  3970 loss =  0.02627\n",
      "epoch =  3980 loss =  0.00576\n",
      "epoch =  3990 loss =  0.01555\n",
      "epoch =  4000 loss =  0.07204\n",
      "epoch =  4010 loss =  0.10760\n",
      "epoch =  4020 loss =  0.48597\n",
      "epoch =  4030 loss =  0.07268\n",
      "epoch =  4040 loss =  0.51319\n",
      "epoch =  4050 loss =  0.01906\n",
      "epoch =  4060 loss =  0.49379\n",
      "epoch =  4070 loss =  0.01807\n",
      "epoch =  4080 loss =  0.22328\n",
      "epoch =  4090 loss =  0.01313\n",
      "epoch =  4100 loss =  0.02747\n",
      "epoch =  4110 loss =  0.00369\n",
      "epoch =  4120 loss =  0.03592\n",
      "epoch =  4130 loss =  0.24130\n",
      "epoch =  4140 loss =  0.00761\n",
      "epoch =  4150 loss =  0.03032\n",
      "epoch =  4160 loss =  0.10100\n",
      "epoch =  4170 loss =  0.02818\n",
      "epoch =  4180 loss =  0.30976\n",
      "epoch =  4190 loss =  0.04787\n",
      "epoch =  4200 loss =  0.46631\n",
      "epoch =  4210 loss =  0.08882\n",
      "epoch =  4220 loss =  0.35827\n",
      "epoch =  4230 loss =  0.23897\n",
      "epoch =  4240 loss =  0.03219\n",
      "epoch =  4250 loss =  0.25903\n",
      "epoch =  4260 loss =  0.11895\n",
      "epoch =  4270 loss =  0.02596\n",
      "epoch =  4280 loss =  0.01213\n",
      "epoch =  4290 loss =  0.01256\n",
      "epoch =  4300 loss =  0.06602\n",
      "epoch =  4310 loss =  0.53971\n",
      "epoch =  4320 loss =  0.34017\n",
      "epoch =  4330 loss =  0.01026\n",
      "epoch =  4340 loss =  0.01576\n",
      "epoch =  4350 loss =  0.72294\n",
      "epoch =  4360 loss =  0.56736\n",
      "epoch =  4370 loss =  0.20828\n",
      "epoch =  4380 loss =  0.01713\n",
      "epoch =  4390 loss =  0.10144\n",
      "epoch =  4400 loss =  0.02112\n",
      "epoch =  4410 loss =  0.46582\n",
      "epoch =  4420 loss =  0.27258\n",
      "epoch =  4430 loss =  0.05300\n",
      "epoch =  4440 loss =  0.03474\n",
      "epoch =  4450 loss =  0.23434\n",
      "epoch =  4460 loss =  0.40440\n",
      "epoch =  4470 loss =  0.16742\n",
      "epoch =  4480 loss =  0.06571\n",
      "epoch =  4490 loss =  0.07048\n",
      "epoch =  4500 loss =  0.00912\n",
      "epoch =  4510 loss =  0.03413\n",
      "epoch =  4520 loss =  0.03369\n",
      "epoch =  4530 loss =  0.02473\n",
      "epoch =  4540 loss =  0.31517\n",
      "epoch =  4550 loss =  0.06724\n",
      "epoch =  4560 loss =  0.03023\n",
      "epoch =  4570 loss =  0.14236\n",
      "epoch =  4580 loss =  0.46083\n",
      "epoch =  4590 loss =  0.59461\n",
      "epoch =  4600 loss =  0.09657\n",
      "epoch =  4610 loss =  0.09813\n",
      "epoch =  4620 loss =  0.01128\n",
      "epoch =  4630 loss =  0.05315\n",
      "epoch =  4640 loss =  0.01962\n",
      "epoch =  4650 loss =  0.07832\n",
      "epoch =  4660 loss =  0.07729\n",
      "epoch =  4670 loss =  0.02812\n",
      "epoch =  4680 loss =  0.26117\n",
      "epoch =  4690 loss =  0.32441\n",
      "epoch =  4700 loss =  0.02097\n",
      "epoch =  4710 loss =  0.14202\n",
      "epoch =  4720 loss =  0.07086\n",
      "epoch =  4730 loss =  0.11867\n",
      "epoch =  4740 loss =  0.09831\n",
      "epoch =  4750 loss =  0.15732\n",
      "epoch =  4760 loss =  0.86273\n",
      "epoch =  4770 loss =  0.25925\n",
      "epoch =  4780 loss =  0.02702\n",
      "epoch =  4790 loss =  0.21849\n",
      "epoch =  4800 loss =  0.32206\n",
      "epoch =  4810 loss =  0.01281\n",
      "epoch =  4820 loss =  0.23867\n",
      "epoch =  4830 loss =  0.04428\n",
      "epoch =  4840 loss =  0.00760\n",
      "epoch =  4850 loss =  0.02319\n",
      "epoch =  4860 loss =  0.12395\n",
      "epoch =  4870 loss =  0.02589\n",
      "epoch =  4880 loss =  0.13538\n",
      "epoch =  4890 loss =  0.05825\n",
      "epoch =  4900 loss =  0.68423\n",
      "epoch =  4910 loss =  0.24767\n",
      "epoch =  4920 loss =  0.17808\n",
      "epoch =  4930 loss =  0.20783\n",
      "epoch =  4940 loss =  0.11239\n",
      "epoch =  4950 loss =  0.01380\n",
      "epoch =  4960 loss =  0.09874\n",
      "epoch =  4970 loss =  0.52487\n",
      "epoch =  4980 loss =  0.13153\n",
      "epoch =  4990 loss =  0.11696\n",
      "epoch =  5000 loss =  0.06662\n",
      "epoch =  5010 loss =  0.13666\n",
      "epoch =  5020 loss =  0.31132\n",
      "epoch =  5030 loss =  0.01443\n",
      "epoch =  5040 loss =  0.01173\n",
      "epoch =  5050 loss =  0.82487\n",
      "epoch =  5060 loss =  0.17584\n",
      "epoch =  5070 loss =  0.03727\n",
      "epoch =  5080 loss =  0.11177\n",
      "epoch =  5090 loss =  0.08827\n",
      "epoch =  5100 loss =  0.03053\n",
      "epoch =  5110 loss =  0.23194\n",
      "epoch =  5120 loss =  0.05558\n",
      "epoch =  5130 loss =  0.17254\n",
      "epoch =  5140 loss =  0.02217\n",
      "epoch =  5150 loss =  0.11535\n",
      "epoch =  5160 loss =  0.15344\n",
      "epoch =  5170 loss =  0.01753\n",
      "epoch =  5180 loss =  0.28840\n",
      "epoch =  5190 loss =  0.26700\n",
      "epoch =  5200 loss =  0.12079\n",
      "epoch =  5210 loss =  0.03501\n",
      "epoch =  5220 loss =  0.13820\n",
      "epoch =  5230 loss =  0.00668\n",
      "epoch =  5240 loss =  0.20826\n",
      "epoch =  5250 loss =  0.16418\n",
      "epoch =  5260 loss =  0.00944\n",
      "epoch =  5270 loss =  0.22846\n",
      "epoch =  5280 loss =  0.03619\n",
      "epoch =  5290 loss =  0.04972\n",
      "epoch =  5300 loss =  0.09698\n",
      "epoch =  5310 loss =  0.02987\n",
      "epoch =  5320 loss =  0.03990\n",
      "epoch =  5330 loss =  0.11207\n",
      "epoch =  5340 loss =  0.06519\n",
      "epoch =  5350 loss =  0.09409\n",
      "epoch =  5360 loss =  0.26176\n",
      "epoch =  5370 loss =  0.47534\n",
      "epoch =  5380 loss =  0.06429\n",
      "epoch =  5390 loss =  0.20497\n",
      "epoch =  5400 loss =  0.21131\n",
      "epoch =  5410 loss =  0.19642\n",
      "epoch =  5420 loss =  0.02517\n",
      "epoch =  5430 loss =  0.05991\n",
      "epoch =  5440 loss =  0.13174\n",
      "epoch =  5450 loss =  0.08233\n",
      "epoch =  5460 loss =  0.02782\n",
      "epoch =  5470 loss =  0.07012\n",
      "epoch =  5480 loss =  0.02705\n",
      "epoch =  5490 loss =  0.02862\n",
      "epoch =  5500 loss =  0.18241\n",
      "epoch =  5510 loss =  0.07927\n",
      "epoch =  5520 loss =  0.05311\n",
      "epoch =  5530 loss =  0.10561\n",
      "epoch =  5540 loss =  0.02304\n",
      "epoch =  5550 loss =  0.28499\n",
      "epoch =  5560 loss =  0.50308\n",
      "epoch =  5570 loss =  0.22185\n",
      "epoch =  5580 loss =  0.18011\n",
      "epoch =  5590 loss =  0.15938\n",
      "epoch =  5600 loss =  0.02951\n",
      "epoch =  5610 loss =  0.02356\n",
      "epoch =  5620 loss =  0.00981\n",
      "epoch =  5630 loss =  0.02023\n",
      "epoch =  5640 loss =  0.01661\n",
      "epoch =  5650 loss =  0.01061\n",
      "epoch =  5660 loss =  0.73263\n",
      "epoch =  5670 loss =  0.31646\n",
      "epoch =  5680 loss =  0.01274\n",
      "epoch =  5690 loss =  0.26251\n",
      "epoch =  5700 loss =  0.11021\n",
      "epoch =  5710 loss =  0.44682\n",
      "epoch =  5720 loss =  0.06475\n",
      "epoch =  5730 loss =  0.08232\n",
      "epoch =  5740 loss =  0.15991\n",
      "epoch =  5750 loss =  0.20841\n",
      "epoch =  5760 loss =  0.06996\n",
      "epoch =  5770 loss =  0.13679\n",
      "epoch =  5780 loss =  0.30643\n",
      "epoch =  5790 loss =  0.00582\n",
      "epoch =  5800 loss =  0.16880\n",
      "epoch =  5810 loss =  0.03727\n",
      "epoch =  5820 loss =  0.16779\n",
      "epoch =  5830 loss =  0.44849\n",
      "epoch =  5840 loss =  0.04744\n",
      "epoch =  5850 loss =  0.06909\n",
      "epoch =  5860 loss =  0.04622\n",
      "epoch =  5870 loss =  0.07074\n",
      "epoch =  5880 loss =  0.03965\n",
      "epoch =  5890 loss =  0.02997\n",
      "epoch =  5900 loss =  0.25329\n",
      "epoch =  5910 loss =  0.12279\n",
      "epoch =  5920 loss =  0.03154\n",
      "epoch =  5930 loss =  0.06777\n",
      "epoch =  5940 loss =  0.02997\n",
      "epoch =  5950 loss =  0.09119\n",
      "epoch =  5960 loss =  0.04024\n",
      "epoch =  5970 loss =  0.16695\n",
      "epoch =  5980 loss =  0.03505\n",
      "epoch =  5990 loss =  0.12793\n",
      "epoch =  6000 loss =  0.02850\n",
      "epoch =  6010 loss =  0.02259\n",
      "epoch =  6020 loss =  0.94262\n",
      "epoch =  6030 loss =  0.15446\n",
      "epoch =  6040 loss =  0.20127\n",
      "epoch =  6050 loss =  0.13795\n",
      "epoch =  6060 loss =  0.42438\n",
      "epoch =  6070 loss =  0.03099\n",
      "epoch =  6080 loss =  0.14382\n",
      "epoch =  6090 loss =  0.15342\n",
      "epoch =  6100 loss =  0.20356\n",
      "epoch =  6110 loss =  0.06755\n",
      "epoch =  6120 loss =  0.15172\n",
      "epoch =  6130 loss =  0.45146\n",
      "epoch =  6140 loss =  0.28575\n",
      "epoch =  6150 loss =  0.05933\n",
      "epoch =  6160 loss =  0.16800\n",
      "epoch =  6170 loss =  0.06875\n",
      "epoch =  6180 loss =  0.04752\n",
      "epoch =  6190 loss =  0.05533\n",
      "epoch =  6200 loss =  0.26319\n",
      "epoch =  6210 loss =  0.34196\n",
      "epoch =  6220 loss =  0.28862\n",
      "epoch =  6230 loss =  0.02127\n",
      "epoch =  6240 loss =  0.06926\n",
      "epoch =  6250 loss =  0.00966\n",
      "epoch =  6260 loss =  0.29055\n",
      "epoch =  6270 loss =  0.09446\n",
      "epoch =  6280 loss =  0.00994\n",
      "epoch =  6290 loss =  0.02490\n",
      "epoch =  6300 loss =  0.16923\n",
      "epoch =  6310 loss =  0.01057\n",
      "epoch =  6320 loss =  0.09019\n",
      "epoch =  6330 loss =  0.20189\n",
      "epoch =  6340 loss =  0.25288\n",
      "epoch =  6350 loss =  0.30007\n",
      "epoch =  6360 loss =  0.60205\n",
      "epoch =  6370 loss =  0.30838\n",
      "epoch =  6380 loss =  0.02337\n",
      "epoch =  6390 loss =  0.39169\n",
      "epoch =  6400 loss =  0.72774\n",
      "epoch =  6410 loss =  0.72138\n",
      "epoch =  6420 loss =  0.00357\n",
      "epoch =  6430 loss =  0.18634\n",
      "epoch =  6440 loss =  0.27844\n",
      "epoch =  6450 loss =  0.35516\n",
      "epoch =  6460 loss =  0.01240\n",
      "epoch =  6470 loss =  0.58100\n",
      "epoch =  6480 loss =  0.08286\n",
      "epoch =  6490 loss =  0.20048\n",
      "epoch =  6500 loss =  0.01897\n",
      "epoch =  6510 loss =  0.54968\n",
      "epoch =  6520 loss =  0.24960\n",
      "epoch =  6530 loss =  0.01578\n",
      "epoch =  6540 loss =  0.08239\n",
      "epoch =  6550 loss =  0.08232\n",
      "epoch =  6560 loss =  0.01841\n",
      "epoch =  6570 loss =  0.11138\n",
      "epoch =  6580 loss =  0.16979\n",
      "epoch =  6590 loss =  0.01762\n",
      "epoch =  6600 loss =  0.01879\n",
      "epoch =  6610 loss =  0.01360\n",
      "epoch =  6620 loss =  0.05031\n",
      "epoch =  6630 loss =  0.63139\n",
      "epoch =  6640 loss =  0.05732\n",
      "epoch =  6650 loss =  0.00488\n",
      "epoch =  6660 loss =  0.02453\n",
      "epoch =  6670 loss =  0.01918\n",
      "epoch =  6680 loss =  0.11638\n",
      "epoch =  6690 loss =  0.74348\n",
      "epoch =  6700 loss =  0.01880\n",
      "epoch =  6710 loss =  0.12403\n",
      "epoch =  6720 loss =  0.00643\n",
      "epoch =  6730 loss =  0.17440\n",
      "epoch =  6740 loss =  0.25764\n",
      "epoch =  6750 loss =  0.48416\n",
      "epoch =  6760 loss =  0.01109\n",
      "epoch =  6770 loss =  0.39404\n",
      "epoch =  6780 loss =  0.00471\n",
      "epoch =  6790 loss =  0.05580\n",
      "epoch =  6800 loss =  0.00441\n",
      "epoch =  6810 loss =  0.00524\n",
      "epoch =  6820 loss =  0.31183\n",
      "epoch =  6830 loss =  0.18691\n",
      "epoch =  6840 loss =  0.05166\n",
      "epoch =  6850 loss =  0.00617\n",
      "epoch =  6860 loss =  0.05896\n",
      "epoch =  6870 loss =  0.00996\n",
      "epoch =  6880 loss =  0.26146\n",
      "epoch =  6890 loss =  0.39974\n",
      "epoch =  6900 loss =  0.01494\n",
      "epoch =  6910 loss =  0.05475\n",
      "epoch =  6920 loss =  0.20370\n",
      "epoch =  6930 loss =  0.23150\n",
      "epoch =  6940 loss =  0.00737\n",
      "epoch =  6950 loss =  0.06799\n",
      "epoch =  6960 loss =  0.01408\n",
      "epoch =  6970 loss =  0.07603\n",
      "epoch =  6980 loss =  0.16346\n",
      "epoch =  6990 loss =  0.14333\n",
      "epoch =  7000 loss =  0.04006\n",
      "epoch =  7010 loss =  0.00332\n",
      "epoch =  7020 loss =  0.25440\n",
      "epoch =  7030 loss =  0.02820\n",
      "epoch =  7040 loss =  0.01739\n",
      "epoch =  7050 loss =  0.00745\n",
      "epoch =  7060 loss =  0.07260\n",
      "epoch =  7070 loss =  0.12172\n",
      "epoch =  7080 loss =  0.10768\n",
      "epoch =  7090 loss =  0.11896\n",
      "epoch =  7100 loss =  0.59257\n",
      "epoch =  7110 loss =  0.21513\n",
      "epoch =  7120 loss =  0.02153\n",
      "epoch =  7130 loss =  0.00928\n",
      "epoch =  7140 loss =  0.10404\n",
      "epoch =  7150 loss =  0.00884\n",
      "epoch =  7160 loss =  0.02318\n",
      "epoch =  7170 loss =  0.54038\n",
      "epoch =  7180 loss =  0.02668\n",
      "epoch =  7190 loss =  0.36806\n",
      "epoch =  7200 loss =  0.31849\n",
      "epoch =  7210 loss =  0.12281\n",
      "epoch =  7220 loss =  0.14569\n",
      "epoch =  7230 loss =  0.05436\n",
      "epoch =  7240 loss =  0.25383\n",
      "epoch =  7250 loss =  0.00204\n",
      "epoch =  7260 loss =  0.24746\n",
      "epoch =  7270 loss =  0.00350\n",
      "epoch =  7280 loss =  0.10596\n",
      "epoch =  7290 loss =  0.02639\n",
      "epoch =  7300 loss =  0.14473\n",
      "epoch =  7310 loss =  0.00696\n",
      "epoch =  7320 loss =  0.14395\n",
      "epoch =  7330 loss =  0.07509\n",
      "epoch =  7340 loss =  0.09147\n",
      "epoch =  7350 loss =  0.12112\n",
      "epoch =  7360 loss =  0.21865\n",
      "epoch =  7370 loss =  0.00520\n",
      "epoch =  7380 loss =  0.01847\n",
      "epoch =  7390 loss =  0.01493\n",
      "epoch =  7400 loss =  0.02437\n",
      "epoch =  7410 loss =  0.03821\n",
      "epoch =  7420 loss =  0.07952\n",
      "epoch =  7430 loss =  0.01804\n",
      "epoch =  7440 loss =  0.12464\n",
      "epoch =  7450 loss =  0.17874\n",
      "epoch =  7460 loss =  0.03603\n",
      "epoch =  7470 loss =  0.02346\n",
      "epoch =  7480 loss =  0.16189\n",
      "epoch =  7490 loss =  0.02269\n",
      "epoch =  7500 loss =  0.04440\n",
      "epoch =  7510 loss =  0.01560\n",
      "epoch =  7520 loss =  0.43795\n",
      "epoch =  7530 loss =  0.11229\n",
      "epoch =  7540 loss =  0.04369\n",
      "epoch =  7550 loss =  0.05500\n",
      "epoch =  7560 loss =  0.02382\n",
      "epoch =  7570 loss =  0.00939\n",
      "epoch =  7580 loss =  0.07031\n",
      "epoch =  7590 loss =  0.02033\n",
      "epoch =  7600 loss =  0.69550\n",
      "epoch =  7610 loss =  0.17741\n",
      "epoch =  7620 loss =  0.14601\n",
      "epoch =  7630 loss =  0.48030\n",
      "epoch =  7640 loss =  0.15436\n",
      "epoch =  7650 loss =  0.23250\n",
      "epoch =  7660 loss =  0.13243\n",
      "epoch =  7670 loss =  0.02359\n",
      "epoch =  7680 loss =  0.04109\n",
      "epoch =  7690 loss =  0.62426\n",
      "epoch =  7700 loss =  0.26503\n",
      "epoch =  7710 loss =  0.00561\n",
      "epoch =  7720 loss =  0.09351\n",
      "epoch =  7730 loss =  0.01374\n",
      "epoch =  7740 loss =  0.05176\n",
      "epoch =  7750 loss =  0.09150\n",
      "epoch =  7760 loss =  0.18932\n",
      "epoch =  7770 loss =  0.16953\n",
      "epoch =  7780 loss =  0.04061\n",
      "epoch =  7790 loss =  0.13487\n",
      "epoch =  7800 loss =  0.08488\n",
      "epoch =  7810 loss =  0.01686\n",
      "epoch =  7820 loss =  0.01992\n",
      "epoch =  7830 loss =  0.03103\n",
      "epoch =  7840 loss =  0.05487\n",
      "epoch =  7850 loss =  0.01851\n",
      "epoch =  7860 loss =  0.24522\n",
      "epoch =  7870 loss =  0.11669\n",
      "epoch =  7880 loss =  0.00748\n",
      "epoch =  7890 loss =  0.03635\n",
      "epoch =  7900 loss =  0.07566\n",
      "epoch =  7910 loss =  0.01970\n",
      "epoch =  7920 loss =  0.01957\n",
      "epoch =  7930 loss =  0.00604\n",
      "epoch =  7940 loss =  0.01905\n",
      "epoch =  7950 loss =  0.18489\n",
      "epoch =  7960 loss =  0.33279\n",
      "epoch =  7970 loss =  0.03122\n",
      "epoch =  7980 loss =  0.27595\n",
      "epoch =  7990 loss =  0.22831\n",
      "epoch =  8000 loss =  0.00324\n",
      "epoch =  8010 loss =  0.49222\n",
      "epoch =  8020 loss =  0.00650\n",
      "epoch =  8030 loss =  0.01199\n",
      "epoch =  8040 loss =  0.13313\n",
      "epoch =  8050 loss =  0.31520\n",
      "epoch =  8060 loss =  0.02657\n",
      "epoch =  8070 loss =  0.28912\n",
      "epoch =  8080 loss =  0.00773\n",
      "epoch =  8090 loss =  0.10603\n",
      "epoch =  8100 loss =  0.02781\n",
      "epoch =  8110 loss =  0.53566\n",
      "epoch =  8120 loss =  0.01474\n",
      "epoch =  8130 loss =  0.02866\n",
      "epoch =  8140 loss =  0.00153\n",
      "epoch =  8150 loss =  0.12285\n",
      "epoch =  8160 loss =  0.08755\n",
      "epoch =  8170 loss =  0.00147\n",
      "epoch =  8180 loss =  0.02612\n",
      "epoch =  8190 loss =  0.01055\n",
      "epoch =  8200 loss =  0.56156\n",
      "epoch =  8210 loss =  0.01864\n",
      "epoch =  8220 loss =  0.18841\n",
      "epoch =  8230 loss =  0.12721\n",
      "epoch =  8240 loss =  0.36192\n",
      "epoch =  8250 loss =  0.05863\n",
      "epoch =  8260 loss =  0.00561\n",
      "epoch =  8270 loss =  0.10183\n",
      "epoch =  8280 loss =  0.01274\n",
      "epoch =  8290 loss =  0.55329\n",
      "epoch =  8300 loss =  0.10643\n",
      "epoch =  8310 loss =  0.01803\n",
      "epoch =  8320 loss =  0.00959\n",
      "epoch =  8330 loss =  0.12593\n",
      "epoch =  8340 loss =  0.16834\n",
      "epoch =  8350 loss =  0.02116\n",
      "epoch =  8360 loss =  0.02076\n",
      "epoch =  8370 loss =  0.33501\n",
      "epoch =  8380 loss =  0.24726\n",
      "epoch =  8390 loss =  0.03305\n",
      "epoch =  8400 loss =  0.01286\n",
      "epoch =  8410 loss =  0.31804\n",
      "epoch =  8420 loss =  0.02568\n",
      "epoch =  8430 loss =  0.00694\n",
      "epoch =  8440 loss =  0.08125\n",
      "epoch =  8450 loss =  0.07991\n",
      "epoch =  8460 loss =  0.00603\n",
      "epoch =  8470 loss =  0.02761\n",
      "epoch =  8480 loss =  0.58775\n",
      "epoch =  8490 loss =  0.93461\n",
      "epoch =  8500 loss =  0.04554\n",
      "epoch =  8510 loss =  0.00206\n",
      "epoch =  8520 loss =  0.00448\n",
      "epoch =  8530 loss =  0.00643\n",
      "epoch =  8540 loss =  0.00842\n",
      "epoch =  8550 loss =  0.04558\n",
      "epoch =  8560 loss =  0.38771\n",
      "epoch =  8570 loss =  0.12093\n",
      "epoch =  8580 loss =  0.18648\n",
      "epoch =  8590 loss =  0.02831\n",
      "epoch =  8600 loss =  0.13404\n",
      "epoch =  8610 loss =  0.01809\n",
      "epoch =  8620 loss =  0.23834\n",
      "epoch =  8630 loss =  0.05843\n",
      "epoch =  8640 loss =  0.04634\n",
      "epoch =  8650 loss =  0.00465\n",
      "epoch =  8660 loss =  0.08043\n",
      "epoch =  8670 loss =  0.00825\n",
      "epoch =  8680 loss =  0.22449\n",
      "epoch =  8690 loss =  0.11084\n",
      "epoch =  8700 loss =  0.00997\n",
      "epoch =  8710 loss =  0.02823\n",
      "epoch =  8720 loss =  0.05846\n",
      "epoch =  8730 loss =  0.17150\n",
      "epoch =  8740 loss =  0.01356\n",
      "epoch =  8750 loss =  0.05326\n",
      "epoch =  8760 loss =  0.00536\n",
      "epoch =  8770 loss =  0.01294\n",
      "epoch =  8780 loss =  0.00593\n",
      "epoch =  8790 loss =  0.00228\n",
      "epoch =  8800 loss =  0.02183\n",
      "epoch =  8810 loss =  0.09058\n",
      "epoch =  8820 loss =  0.04771\n",
      "epoch =  8830 loss =  0.02309\n",
      "epoch =  8840 loss =  0.00890\n",
      "epoch =  8850 loss =  0.01799\n",
      "epoch =  8860 loss =  0.09325\n",
      "epoch =  8870 loss =  0.11741\n",
      "epoch =  8880 loss =  0.01057\n",
      "epoch =  8890 loss =  0.05196\n",
      "epoch =  8900 loss =  0.02515\n",
      "epoch =  8910 loss =  0.01504\n",
      "epoch =  8920 loss =  0.01210\n",
      "epoch =  8930 loss =  0.08061\n",
      "epoch =  8940 loss =  0.58894\n",
      "epoch =  8950 loss =  0.14234\n",
      "epoch =  8960 loss =  0.04211\n",
      "epoch =  8970 loss =  0.55255\n",
      "epoch =  8980 loss =  0.01601\n",
      "epoch =  8990 loss =  0.11728\n",
      "epoch =  9000 loss =  0.47024\n",
      "epoch =  9010 loss =  0.31474\n",
      "epoch =  9020 loss =  0.55617\n",
      "epoch =  9030 loss =  0.13094\n",
      "epoch =  9040 loss =  0.51323\n",
      "epoch =  9050 loss =  0.00327\n",
      "epoch =  9060 loss =  0.00673\n",
      "epoch =  9070 loss =  0.04301\n",
      "epoch =  9080 loss =  0.51451\n",
      "epoch =  9090 loss =  0.00783\n",
      "epoch =  9100 loss =  0.00342\n",
      "epoch =  9110 loss =  0.01138\n",
      "epoch =  9120 loss =  0.00903\n",
      "epoch =  9130 loss =  0.00461\n",
      "epoch =  9140 loss =  0.01131\n",
      "epoch =  9150 loss =  0.01335\n",
      "epoch =  9160 loss =  0.18548\n",
      "epoch =  9170 loss =  0.05170\n",
      "epoch =  9180 loss =  0.00793\n",
      "epoch =  9190 loss =  0.07450\n",
      "epoch =  9200 loss =  0.00918\n",
      "epoch =  9210 loss =  0.01672\n",
      "epoch =  9220 loss =  0.00704\n",
      "epoch =  9230 loss =  0.03194\n",
      "epoch =  9240 loss =  0.11873\n",
      "epoch =  9250 loss =  0.17473\n",
      "epoch =  9260 loss =  0.13774\n",
      "epoch =  9270 loss =  0.04205\n",
      "epoch =  9280 loss =  0.01339\n",
      "epoch =  9290 loss =  0.22257\n",
      "epoch =  9300 loss =  0.21507\n",
      "epoch =  9310 loss =  0.02073\n",
      "epoch =  9320 loss =  0.16411\n",
      "epoch =  9330 loss =  0.49361\n",
      "epoch =  9340 loss =  0.07831\n",
      "epoch =  9350 loss =  0.03223\n",
      "epoch =  9360 loss =  0.00516\n",
      "epoch =  9370 loss =  0.11203\n",
      "epoch =  9380 loss =  0.07187\n",
      "epoch =  9390 loss =  0.06519\n",
      "epoch =  9400 loss =  0.10316\n",
      "epoch =  9410 loss =  0.17336\n",
      "epoch =  9420 loss =  0.60330\n",
      "epoch =  9430 loss =  0.02776\n",
      "epoch =  9440 loss =  0.09844\n",
      "epoch =  9450 loss =  0.21483\n",
      "epoch =  9460 loss =  0.05428\n",
      "epoch =  9470 loss =  0.11484\n",
      "epoch =  9480 loss =  0.11107\n",
      "epoch =  9490 loss =  0.01414\n",
      "epoch =  9500 loss =  0.01103\n",
      "epoch =  9510 loss =  0.02934\n",
      "epoch =  9520 loss =  0.21673\n",
      "epoch =  9530 loss =  0.01450\n",
      "epoch =  9540 loss =  0.03597\n",
      "epoch =  9550 loss =  0.21970\n",
      "epoch =  9560 loss =  0.03082\n",
      "epoch =  9570 loss =  0.01296\n",
      "epoch =  9580 loss =  0.01378\n",
      "epoch =  9590 loss =  0.27271\n",
      "epoch =  9600 loss =  0.54468\n",
      "epoch =  9610 loss =  0.03099\n",
      "epoch =  9620 loss =  0.08372\n",
      "epoch =  9630 loss =  0.00205\n",
      "epoch =  9640 loss =  0.37184\n",
      "epoch =  9650 loss =  0.00577\n",
      "epoch =  9660 loss =  0.04774\n",
      "epoch =  9670 loss =  0.02325\n",
      "epoch =  9680 loss =  0.15228\n",
      "epoch =  9690 loss =  0.01264\n",
      "epoch =  9700 loss =  0.07338\n",
      "epoch =  9710 loss =  0.13811\n",
      "epoch =  9720 loss =  0.04190\n",
      "epoch =  9730 loss =  0.19211\n",
      "epoch =  9740 loss =  0.23035\n",
      "epoch =  9750 loss =  0.00383\n",
      "epoch =  9760 loss =  0.35380\n",
      "epoch =  9770 loss =  0.26847\n",
      "epoch =  9780 loss =  0.01742\n",
      "epoch =  9790 loss =  0.43732\n",
      "epoch =  9800 loss =  0.11060\n",
      "epoch =  9810 loss =  0.03306\n",
      "epoch =  9820 loss =  0.01166\n",
      "epoch =  9830 loss =  0.16878\n",
      "epoch =  9840 loss =  0.06929\n",
      "epoch =  9850 loss =  0.01612\n",
      "epoch =  9860 loss =  0.26949\n",
      "epoch =  9870 loss =  0.00694\n",
      "epoch =  9880 loss =  0.24386\n",
      "epoch =  9890 loss =  0.02984\n",
      "epoch =  9900 loss =  0.00941\n",
      "epoch =  9910 loss =  0.01304\n",
      "epoch =  9920 loss =  0.02055\n",
      "epoch =  9930 loss =  0.19684\n",
      "epoch =  9940 loss =  0.25409\n",
      "epoch =  9950 loss =  0.24505\n",
      "epoch =  9960 loss =  0.04553\n",
      "epoch =  9970 loss =  0.33800\n",
      "epoch =  9980 loss =  0.00591\n",
      "epoch =  9990 loss =  0.14142\n",
      "epoch =  9999 loss =  0.07746\n",
      "accuracy on test set = 0.9333333333333333\n",
      "confusion matrix = \n",
      "[[30.  1.  1.]\n",
      " [ 2. 27.  0.]\n",
      " [ 3.  0. 41.]]\n"
     ]
    }
   ],
   "source": [
    "C,acc, model=NeuralNetwork(Xtrain=Xtrain,\n",
    "              Ytrain=Ytrain,\n",
    "              Xtest=Xtest,\n",
    "              Ytest=Ytest,\n",
    "              hidden_layer_sizes=[10,10],\n",
    "              batchsize=50, \n",
    "              nepochs=10000,\n",
    "              lrate=.001)\n",
    "\n",
    "print(\"accuracy on test set = \"+str(acc))\n",
    "print(\"confusion matrix = \")\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a284dd9-b0e9-44c5-b01b-44cf1cfb82dc",
   "metadata": {},
   "source": [
    "Note that we can save the model which has all information about the trained network.\n",
    "I've added a module to the model that computes the output probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28db6425-3a7f-4388-a2c8-001f19828b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.872 0.114 0.014] 0\n",
      "[0.229 0.002 0.769] 2\n",
      "[0.123 0.874 0.003] 1\n",
      "[0.947 0.024 0.03 ] 0\n",
      "[0.012 0.    0.988] 2\n",
      "[0.247 0.001 0.752] 2\n",
      "[0.5   0.002 0.497] 2\n",
      "[0.532 0.    0.468] 2\n",
      "[0.155 0.841 0.004] 1\n",
      "[0.106 0.892 0.002] 1\n",
      "[0.016 0.    0.984] 2\n",
      "[0.875 0.072 0.053] 0\n",
      "[0.05  0.    0.949] 2\n",
      "[0.064 0.936 0.   ] 1\n",
      "[0.069 0.93  0.002] 1\n",
      "[0.09  0.908 0.002] 1\n",
      "[0.859 0.118 0.023] 0\n",
      "[0.05  0.95  0.001] 1\n",
      "[0.842 0.132 0.026] 0\n",
      "[0.008 0.    0.992] 2\n",
      "[0.014 0.985 0.   ] 1\n",
      "[0.057 0.942 0.001] 1\n",
      "[0.402 0.005 0.593] 2\n",
      "[0.911 0.085 0.004] 0\n",
      "[0.068 0.93  0.001] 1\n",
      "[0.909 0.083 0.008] 0\n",
      "[0.092 0.001 0.907] 2\n",
      "[0.627 0.356 0.018] 0\n",
      "[0.106 0.893 0.001] 1\n",
      "[0.069 0.93  0.001] 1\n",
      "[0.408 0.586 0.006] 0\n",
      "[0.249 0.005 0.747] 2\n",
      "[0.962 0.024 0.015] 0\n",
      "[0.056 0.    0.944] 2\n",
      "[0.029 0.971 0.   ] 1\n",
      "[0.743 0.227 0.03 ] 0\n",
      "[0.03 0.97 0.  ] 1\n",
      "[0.165 0.833 0.002] 1\n",
      "[0.037 0.962 0.   ] 1\n",
      "[0.887 0.054 0.058] 0\n",
      "[0.03 0.97 0.  ] 1\n",
      "[0.577 0.419 0.005] 0\n",
      "[0.132 0.867 0.001] 1\n",
      "[0.163 0.835 0.002] 0\n",
      "[0.094 0.    0.906] 2\n",
      "[0.67  0.234 0.096] 0\n",
      "[0.02  0.979 0.   ] 1\n",
      "[0.018 0.    0.982] 2\n",
      "[0.516 0.062 0.422] 2\n",
      "[0.009 0.    0.99 ] 2\n",
      "[0.032 0.968 0.001] 1\n",
      "[0.609 0.035 0.357] 0\n",
      "[0.069 0.93  0.001] 1\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Print output probabilities and true class for each test feature vector\n",
    "#\n",
    "for i in range(Xtest.shape[0]):\n",
    "    x=torch.tensor(Xtest[i,:],dtype=torch.float)\n",
    "    p=model.probability_vector(x).detach().numpy().round(3)\n",
    "    y=Ytest[i]\n",
    "    print(str(p)+\" \" +str(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4540762b-23a2-4a20-8b4d-afb8865d882b",
   "metadata": {},
   "source": [
    "We can also interrogate the model and determine the parameters that define it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c75c55c5-1c70-4bba-acd4-5f07c72313c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyNetwork(\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=7, out_features=10, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (3): Sigmoid()\n",
       "    (4): Linear(in_features=10, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bd618f6-14b4-468f-9506-1f39febfdb4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Linear(in_features=7, out_features=10, bias=True)\n",
       "  (1): Sigmoid()\n",
       "  (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (3): Sigmoid()\n",
       "  (4): Linear(in_features=10, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4649cbb-1db6-441b-984c-8a38b224a098",
   "metadata": {},
   "source": [
    "model.named_parameters() gives a generator for getting all of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f076eff0-efda-4034-813f-7cd34666aff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generator"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccd76511-e773-43d3-9aa2-e973170a5cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.weight\n",
      "tensor([[ 0.8021,  0.0494, -0.2951, -0.0136,  0.0548, -1.6866, -0.7352],\n",
      "        [-0.3765, -0.5064, -0.1677,  0.1554, -0.4876,  0.4046, -0.1068],\n",
      "        [ 0.8944, -0.0235, -0.3187, -0.5419,  0.6771, -0.8545, -1.1146],\n",
      "        [-0.8285,  0.1792,  1.8355,  0.4411,  0.8648, -0.2039,  0.2447],\n",
      "        [-0.8175,  0.1900,  1.3158,  0.3791,  0.5329, -0.3060,  0.4472],\n",
      "        [-0.5211,  0.5258,  1.6666,  0.5966,  0.7532, -1.0153, -1.0069],\n",
      "        [ 0.5549,  0.2421, -0.0474,  0.4468, -0.0864, -0.0300,  0.2585],\n",
      "        [ 0.8337, -0.1084, -1.1371, -0.2190, -0.2800, -1.5585,  0.0780],\n",
      "        [ 0.3233,  0.2681,  0.1780, -0.0154,  0.4824,  0.3229,  0.4113],\n",
      "        [ 0.9048, -0.6419, -1.1359, -0.6985, -0.6883,  0.0300,  0.5410]])\n",
      "layers.0.bias\n",
      "tensor([-0.7961,  0.1077, -0.6294,  1.5384,  1.4257,  0.9897, -0.1468, -1.4514,\n",
      "        -0.1597, -1.0506])\n",
      "layers.2.weight\n",
      "tensor([[ 1.8540, -0.3485,  2.8522, -1.8906, -1.5172,  1.1286, -0.1875,  1.2515,\n",
      "          0.0147,  2.5715],\n",
      "        [-0.6886,  0.5111, -0.3994,  2.2377,  1.9486,  3.3683,  0.5540, -1.2471,\n",
      "          0.4616, -2.4679],\n",
      "        [-1.1527,  0.2733, -1.9654,  1.2429,  0.9799, -1.0006,  0.2536, -0.3377,\n",
      "          0.2720, -1.3263],\n",
      "        [-0.4822,  0.5348, -0.5912,  2.2240,  1.7628,  3.7491,  0.7455, -1.5065,\n",
      "          0.5081, -2.3202],\n",
      "        [ 0.7118, -0.1840,  1.6245, -3.3346, -2.9649, -0.5218, -0.5719,  0.7551,\n",
      "         -0.2604,  2.8421],\n",
      "        [ 0.5532, -0.4994,  1.0369, -3.0288, -2.7447, -2.2891, -0.5448,  1.5649,\n",
      "         -0.2550,  2.3862],\n",
      "        [ 0.7047, -0.0311,  0.9562, -2.5710, -3.0531, -2.0969, -0.6047,  1.3620,\n",
      "         -0.1297,  2.4507],\n",
      "        [ 0.5640, -0.4214, -0.1684, -1.3347, -0.5764, -3.8417, -0.1004,  1.3620,\n",
      "         -0.4459,  1.8577],\n",
      "        [ 1.5926, -0.3138,  2.7646, -1.2526, -1.2610,  0.9854, -0.3467,  0.8093,\n",
      "         -0.3405,  2.0396],\n",
      "        [-1.5711,  0.1314, -2.3203,  1.5367,  1.4771, -1.0122,  0.3900, -0.6990,\n",
      "         -0.1408, -1.8555]])\n",
      "layers.2.bias\n",
      "tensor([-0.4060,  0.7056,  0.4167,  0.3936, -0.3435, -0.2204, -0.3663, -0.1161,\n",
      "        -0.2776,  0.2737])\n",
      "layers.4.weight\n",
      "tensor([[ 4.9277e-01,  7.1362e-01, -8.2226e-01,  7.4256e-01,  6.2457e-01,\n",
      "          3.3525e-03,  1.8458e-01, -1.7884e+00,  3.7788e-01, -1.4784e+00],\n",
      "        [ 6.5833e-01, -3.4468e+00, -1.5211e+00, -3.5990e+00,  9.5947e-01,\n",
      "          1.3851e+00,  1.8617e+00,  2.5210e+00,  3.1911e-01, -2.3732e+00],\n",
      "        [-2.1820e+00,  1.9389e+00,  1.6551e+00,  1.9555e+00, -2.6627e+00,\n",
      "         -2.6902e+00, -2.8408e+00, -1.2526e+00, -1.3731e+00,  2.7866e+00]])\n",
      "layers.4.bias\n",
      "tensor([-0.1879,  0.1933,  0.6422])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    print(param.data)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "25c41e35-a732-4f51-a4ff-4c7fe28c680f",
   "metadata": {},
   "source": [
    "We added a method to the model/network that allows us to see the output in stages.\n",
    "I'd like to make sure I know what's going on by seeing whether I can reproduce these outputs from the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4ae08e1-361f-4e07-8508-215f79e4538b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = \n",
      "tensor([14.5900, 14.2800,  0.8993,  5.3510,  3.3330,  4.1850,  4.7810])\n",
      "\n",
      "\n",
      "output 0\n",
      "tensor([  0.8829, -12.3787,   2.2498,  -0.7800,  -1.9421,  -0.9666,  14.5773,\n",
      "         -0.1132,  13.3893,  -1.3572], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "output 1\n",
      "tensor([7.0743e-01, 4.2074e-06, 9.0463e-01, 3.1431e-01, 1.2542e-01, 2.7556e-01,\n",
      "        1.0000e+00, 4.7172e-01, 1.0000e+00, 2.0469e-01],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "\n",
      "\n",
      "output 2\n",
      "tensor([ 3.9560,  1.6552, -1.8440,  1.5390,  0.1717, -0.3913, -0.3618, -0.9434,\n",
      "         3.1818, -3.0077], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "output 3\n",
      "tensor([0.9812, 0.8396, 0.1366, 0.8233, 0.5428, 0.4034, 0.4105, 0.2802, 0.9601,\n",
      "        0.0471], grad_fn=<SigmoidBackward0>)\n",
      "\n",
      "\n",
      "output 4\n",
      "tensor([ 1.6020, -2.4806, -3.2698], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "output 5\n",
      "tensor([0.9761, 0.0165, 0.0075], grad_fn=<DivBackward0>)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor(Xtest[0],dtype=torch.float)\n",
    "model.output_in_stages(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66725589-9f9a-4a6f-84e9-26db1cb7b3eb",
   "metadata": {},
   "source": [
    "Copy all of the parameters into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13d64fb-86f5-4479-9b61-8a32830e057d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LTENSORS=[]\n",
    "p=model.named_parameters()\n",
    "for u,v in p:\n",
    "    LTENSORS.append(v.data)\n",
    "print(LTENSORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91c56e5-5489-48e3-9d8e-b3f42184e4f7",
   "metadata": {},
   "source": [
    "Try to reproduce what the linear transformation does in the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ccc266-0d78-44d9-b782-f2797dc15d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "W=LTENSORS[0]\n",
    "B=LTENSORS[1]\n",
    "x=torch.tensor(Xtest[0],dtype=torch.float)\n",
    "print(W.size())\n",
    "print(B.size())\n",
    "print(x.size())\n",
    "y=torch.matmul(W,x)+B\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efd5bd0-e7ef-4667-a3d2-7a6139a722c7",
   "metadata": {},
   "source": [
    "Check the next tensor in which the sigmoid function is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8e1b5fe-d5ed-47c6-a0e4-61d0b5bc99e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39mtorch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39m\u001b[43my\u001b[49m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "1/(1+torch.exp(-y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2db8a7-141d-4e4c-aaaf-ed5809399d77",
   "metadata": {},
   "source": [
    "Finally, check that when we apply the softmax transformation to the final model output we get the probability vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc52f9ce-0742-4a59-bc60-648181baaff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=model(x)\n",
    "torch.exp(y)/torch.sum(torch.exp(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bd43c0-879d-4841-9e7c-a57b874752a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
